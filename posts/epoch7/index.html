<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 7: Makemore, Part 4 | Memo Box</title><meta property="og:title" content="Epoch 7: Makemore, Part 4"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch7/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.a6a3eb7060356ad051e321fafd01ddc5b490e51f57e274b266d67cdeaea302f4.css integrity="sha256-pqPrcGA1atBR4yH6/QHdxbSQ5R9X4nSyZtZ83q6jAvQ=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css><script src=https://cdn.jsdelivr.net/npm/glightbox/dist/js/glightbox.min.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch7/>Epoch 7: Makemore, Part 4</a></li></ul></nav></header><main><h1>Epoch 7: Makemore, Part 4</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-mlp-part-3>Content: MLP Part 3</a></li><li><a href=#cross_entropy>cross_entropy</a></li><li><a href=#mini-batch>Mini Batch</a></li><li><a href=#determining-the-right-update-size-of-w>Determining the right update size of W</a></li><li><a href=#split-dataset-into-3-splits>split dataset into 3 splits</a></li><li><a href=#plot-c>plot C</a></li><li><a href=#sampling>Sampling</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 class=no-counter id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, in the previous epoch we learnt how to make the backward pass of the bigram and make a one layer neural net, which the result of the training would be approximately equal to &ldquo;normalizing the counts&rdquo; approach, then we began to build mlp and context of 3 chars to predict the 4th chars. We continue to build the mlp of Makemore in this epoch.</p><h2 class=no-counter id=content-mlp-part-3>Content: MLP Part 3</h2><p>We have already created the second layer of the mlp with tanh:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span></code></pre></div><br>Now we need to create the third (last layer)<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>,<span style=color:#ae81ff>27</span>)
</span></span><span style=display:flex><span>b2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>27</span>)
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>logits<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([32, 27])</span>
</span></span></code></pre></div><p>and</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>prob <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims <span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>prob[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>32</span>), Y]
</span></span></code></pre></div><br>This shows the probability of feeding with the context index and getting the 4th letter.<br>Now at last we calculate the loss:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>prob[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>32</span>), Y]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(15.9918)</span>
</span></span></code></pre></div><p>We need to minimize this number&mldr;<br>to understand the reason why we used neg log mean, read the previous epoches.<br></p><p><em>Summary</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#init</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>367452352</span>)
</span></span><span style=display:flex><span>C <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>2</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>W1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>100</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>b1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>100</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>W2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>27</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>b2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>27</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>parameters <span style=color:#f92672>=</span> [C, W1, b1, W2, b2]
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>    p<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sum(p<span style=color:#f92672>.</span>nelement() <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters)
</span></span><span style=display:flex><span><span style=color:#75715e>#3481</span>
</span></span></code></pre></div><blockquote><p>nelement() counts the elements of a matrix</p></blockquote><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>emb <span style=color:#f92672>=</span> C[X]
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>prob <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>prob[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>32</span>), Y]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>loss
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(15.9918)</span>
</span></span></code></pre></div><br><blockquote><p><strong>Note</strong>:In the bigram, we used one-hot encoding to convert indices into numerical vectors, and then multiplied them by W. This multiplication effectively selects one row of W, because each one-hot vector contains a single 1.<br><br></p><p>In the MLP model, Instead of one-hot vectors, we directly index into the embedding matrix using C[X], which chooses the corresponding rows of C. This is more efficient and mathematically equivalent to one-hot encoding followed by a matrix multiplication.<br></p></blockquote><h2 class=no-counter id=cross_entropy>cross_entropy</h2><p>Instead of calulating counts, probs, and loss(neg log mean), for efficiency we can use cross_entropy to let pytorch compute logits to loss!<br><em>logits(raw score) -> loss</em><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>emb <span style=color:#f92672>=</span> C[X]
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span><span style=color:#75715e># counts = logits.exp()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># prob = counts / counts.sum(1, keepdims=True)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Y)
</span></span></code></pre></div><br>Let's make our autograd:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    emb <span style=color:#f92672>=</span> C[X]
</span></span><span style=display:flex><span>    h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>    <span style=color:#75715e># counts = logits.exp()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># prob = counts / counts.sum(1, keepdims=True)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Y)
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward pass </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><br><blockquote><p>the result of the loss after many trainings, is around, 0.25</p><p>The reason is because some contexts (&rsquo;. . .&rsquo;) have different 4th chars.
So the autograd cannot determine one specific 4th letter for all specific contexts such as<br>&lsquo;. . .&rsquo; context.</p></blockquote><p><br><br></p><h2 class=no-counter id=mini-batch>Mini Batch</h2><p>Now that we want to use all the names/words, there are 228146 contexts, and it is more efficient to use mini batches instead.<br>By randomly select random portion of dataset, and only do forward pass, backward pass, and updates those mini batches.<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], (<span style=color:#ae81ff>32</span>,))
</span></span></code></pre></div><p><br><br>This creates 32 random ints, between 0 and the X.shape[0] which is 228146.<br>Now our new autograd is:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], (<span style=color:#ae81ff>32</span>,))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    emb <span style=color:#f92672>=</span> C[X[ix]] <span style=color:#75715e>#changed</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>    <span style=color:#75715e># counts = logits.exp()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># prob = counts / counts.sum(1, keepdims=True)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Y[ix]) <span style=color:#75715e>#changed </span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward pass </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><blockquote><p>Note: The quality of gradient is lower, but the gradient direction is still a good approximation if we do it in small size with more steps.</p></blockquote><br><h2 class=no-counter id=determining-the-right-update-size-of-w>Determining the right update size of W</h2><p>To do so we need to create a plot, update W with different step sizes, and compare the loss on the plot.<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lre <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0.001</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1000</span>)
</span></span></code></pre></div><p>This creates 1000 steps, between 0.001 to 1<br>e.g.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#ae81ff>0.0010</span>, <span style=color:#ae81ff>0.0020</span>, <span style=color:#ae81ff>0.0030</span>,<span style=color:#f92672>...</span>
</span></span></code></pre></div><p>But let&rsquo;s use torch.logspace(-3, 0, 1000),<br>This creates 1000 steps, between 10^-3 to 10^0<br>Simply because learning rate effects are multiplicative, not additive<br><br></p><p>outside the loop:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>logsi <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>logspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1000</span>)
</span></span><span style=display:flex><span>lrs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>allloss <span style=color:#f92672>=</span> []
</span></span></code></pre></div><br>inside forward pass:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lr <span style=color:#f92672>=</span> logsi[i]
</span></span><span style=display:flex><span>allloss<span style=color:#f92672>.</span>append(loss<span style=color:#f92672>.</span>item())
</span></span></code></pre></div><br>after all:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lrs<span style=color:#f92672>.</span>append(lr)
</span></span></code></pre></div><br>full code:<br><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>logs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>logspace(<span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1000</span>)
</span></span><span style=display:flex><span>lrs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>allloss <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], (<span style=color:#ae81ff>32</span>,))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    lr <span style=color:#f92672>=</span> logs[i]
</span></span><span style=display:flex><span>    emb <span style=color:#f92672>=</span> C[X[ix]]
</span></span><span style=display:flex><span>    h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>    <span style=color:#75715e># counts = logits.exp()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># prob = counts / counts.sum(1, keepdims=True)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Y[ix])
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>    allloss<span style=color:#f92672>.</span>append(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward pass </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span>lr <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    lrs<span style=color:#f92672>.</span>append(lr)
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><p>Now the plot:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(lrs,allloss)
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch7-plot.png alt=epoch7-plot><br><br></p><p>x: lrs, y: allloss<br>As you see the best learning rate is in the area of 0.1 and 0.2<br>So let&rsquo;s choose 0.1 and run many steps, to get more precise result, after many steps, we use 10x lower step size: 0.01 and then run the code and check the loss, and then continue the training&mldr;<br><br></p><h2 class=no-counter id=split-dataset-into-3-splits>split dataset into 3 splits</h2><p>We split our dataset into 3 splits:<br>Training, validation, and test splits<br><br><br></p><p><em>Learning the weights</em>: by gradient descent <strong>(training set)</strong> 80% of our dataset<br><br></p><p><em>Choosing hyperparameters</em>: by observing results <strong>(validation set)</strong>, we compare the model sizes, learning rates, embedding size, strength of regularization and decide when to stop the training,<br>Note: no backward() in this set at all<br></p><p><em>Final performance</em>: by unbiased and untouched <strong>test set</strong><br>Note: We are only allowed to check loss in test set only a few times, to prevent overfitting and bias of tester.</p><p>Now let&rsquo;s make our new autograd with these splits:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#init  </span>
</span></span><span style=display:flex><span>block_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> 
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_dataset</span>(words):
</span></span><span style=display:flex><span>  X, Y <span style=color:#f92672>=</span> [], []
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    context <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> block_size
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch <span style=color:#f92672>in</span> w <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.&#39;</span>:
</span></span><span style=display:flex><span>      ix <span style=color:#f92672>=</span> stoi[ch]
</span></span><span style=display:flex><span>      X<span style=color:#f92672>.</span>append(context)
</span></span><span style=display:flex><span>      Y<span style=color:#f92672>.</span>append(ix)
</span></span><span style=display:flex><span>      context <span style=color:#f92672>=</span> context[<span style=color:#ae81ff>1</span>:] <span style=color:#f92672>+</span> [ix] <span style=color:#75715e># sliding window</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(X)
</span></span><span style=display:flex><span>  Y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(Y)
</span></span><span style=display:flex><span>  print(X<span style=color:#f92672>.</span>shape, Y<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> X, Y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>shuffle(words)
</span></span><span style=display:flex><span>n1 <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.8</span><span style=color:#f92672>*</span>len(words))
</span></span><span style=display:flex><span>n2 <span style=color:#f92672>=</span> int(<span style=color:#ae81ff>0.9</span><span style=color:#f92672>*</span>len(words))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Xtr, Ytr <span style=color:#f92672>=</span> build_dataset(words[:n1])
</span></span><span style=display:flex><span>Xdev, Ydev <span style=color:#f92672>=</span> build_dataset(words[n1:n2])
</span></span><span style=display:flex><span>Xte, Yte <span style=color:#f92672>=</span> build_dataset(words[n2:])
</span></span></code></pre></div><br>First we shuffle or randomize the words.<br><p>We can feed this function with n1(80%) to achieve the trainingg set(Xtr, Ytr), or from n1 80% to n2 90% = 10% for the dev set (Xdev, Ydev) or the rest of the n2 90% to 100% = 10% for the test set (Xte, Yte)<br><br></p><br>Same parameters(we can change the neuron counts)<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>2147483647</span>)
</span></span><span style=display:flex><span>C <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>10</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>W1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>200</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>b1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>200</span>, generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>W2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>200</span>, <span style=color:#ae81ff>27</span>), generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>b2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>27</span>, generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>parameters <span style=color:#f92672>=</span> [C, W1, b1, W2, b2]
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, Xtr<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], (<span style=color:#ae81ff>32</span>,))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#lr = logs[i]</span>
</span></span><span style=display:flex><span>    emb <span style=color:#f92672>=</span> C[Xtr[ix]]
</span></span><span style=display:flex><span>    h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>    <span style=color:#75715e># counts = logits.exp()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># prob = counts / counts.sum(1, keepdims=True)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># loss = -prob[torch.arange(32), Y].log().mean()</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Ytr[ix])
</span></span><span style=display:flex><span>    <span style=color:#75715e># print(loss.item())</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># allloss.append(loss.item())</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward pass </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> parameters:
</span></span><span style=display:flex><span>    <span style=color:#75715e>#   p.data += -lr * p.grad</span>
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.01</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># lrs.append(lr)</span>
</span></span><span style=display:flex><span>print(loss<span style=color:#f92672>.</span>item()) 
</span></span></code></pre></div><blockquote><p>As you may noticed we just changed the X with Xtr and Y with Ytr. To calcualte the X and Y of dev and test sets, we can put these outside the loop:<br></p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># validation loss</span>
</span></span><span style=display:flex><span>emb <span style=color:#f92672>=</span> C[Xdev] <span style=color:#75715e># (32, 3, 10)</span>
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1) <span style=color:#75715e># (32, 100)</span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2 <span style=color:#75715e># (32, 27)</span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Ydev)
</span></span><span style=display:flex><span>loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># test loss</span>
</span></span><span style=display:flex><span>emb <span style=color:#f92672>=</span> C[Xte] <span style=color:#75715e># (32, 3, 10) emb dim C = 10</span>
</span></span><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1) <span style=color:#75715e># (32, 100)</span>
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2 <span style=color:#75715e># (32, 27)</span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Yte)
</span></span><span style=display:flex><span>loss
</span></span></code></pre></div><br><blockquote><p>Note: we are printing the validation and test loss outside the loop of training set.<br></p></blockquote><h2 class=no-counter id=plot-c>plot C</h2><p>Since our C embedded dim is 2 for each char, we can consideer each as an axis, and plot the char with that 2 dims.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># visualize dimensions 0 and 1 of the embedding matrix C for all characters</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>8</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(C[:,<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>data, C[:,<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>data, s<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(C<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>text(C[i,<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>item(), C[i,<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>item(), itos[i], ha<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;center&#34;</span>, va<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;center&#34;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;white&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#e6db74>&#39;minor&#39;</span>)
</span></span></code></pre></div><p>plotting parameters, shows valuable data such as: Inputs that are near each other have similar likelihood of occurrence, the closer they are, the more interchangeable they are.</p><p><img src=https://auroramonet.github.io/memo/img/epoch7-plot2.png alt=epoch7-plot2><br></p><h2 class=no-counter id=sampling>Sampling</h2><p>After we trained our model, let&rsquo;s pick some names as samples:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>2147483647</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    context <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> block_size
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>      emb <span style=color:#f92672>=</span> C[torch<span style=color:#f92672>.</span>tensor([context])]
</span></span><span style=display:flex><span>      h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span><span style=display:flex><span>      logits <span style=color:#f92672>=</span> h <span style=color:#f92672>@</span> W2 <span style=color:#f92672>+</span> b2
</span></span><span style=display:flex><span>      probs <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(logits, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>      ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(probs, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, generator<span style=color:#f92672>=</span>g)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>      context <span style=color:#f92672>=</span> context[<span style=color:#ae81ff>1</span>:] <span style=color:#f92672>+</span> [ix]
</span></span><span style=display:flex><span>      out<span style=color:#f92672>.</span>append(ix)
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(itos[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> out))
</span></span></code></pre></div><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><h3 id=google-colab-notebook>Google Colab Notebook</h3><p><a href="https://colab.research.google.com/drive/1YIfmkftLrz6MPTOO9Vwqrop2Q5llHIGK?usp=sharing#scrollTo=TQUMmgRrdRIA">Google Colab: Makemore Part 4</a></p><div class=time><time datetime=2026-01-10>2026-01-10&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("main article img, main .content img, main img");e.forEach(function(e){if(e.parentElement.tagName==="A")return;const t=document.createElement("a");t.href=e.src,t.setAttribute("data-glightbox",""),t.setAttribute("data-gallery","gallery"),e.alt&&t.setAttribute("data-title",e.alt),e.parentNode.insertBefore(t,e),t.appendChild(e)});const n=GLightbox({selector:"a[data-glightbox]",touchNavigation:!0,loop:!0,autoplayVideos:!1}),t=document.querySelectorAll('code.language-python-copy, code[class*="python-copy"], code[data-lang="python-copy"]');t.forEach(function(e){let o=e.textContent||e.innerText;const n=e.closest("pre");if(!n)return;const i=n.parentElement;if(!i)return;n.style.display="none";const s=document.createElement("div");s.className="code-copy-container";const t=document.createElement("button");t.className="code-copy-button",t.textContent="Copy Code",t.setAttribute("aria-label","Copy code to clipboard"),t.addEventListener("click",function(){if(navigator.clipboard&&navigator.clipboard.writeText)navigator.clipboard.writeText(o).then(function(){t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}).catch(function(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)});else{const e=document.createElement("textarea");e.value=o,e.style.position="fixed",e.style.opacity="0",document.body.appendChild(e),e.select();try{document.execCommand("copy"),t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}catch(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)}document.body.removeChild(e)}}),s.appendChild(t),i.insertBefore(s,n)})})</script></body></html>
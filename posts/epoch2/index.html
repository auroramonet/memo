<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 2: Micrograd, Part 2 | Memo Box</title><meta property="og:title" content="Epoch 2: Micrograd, Part 2"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch2/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.a6a3eb7060356ad051e321fafd01ddc5b490e51f57e274b266d67cdeaea302f4.css integrity="sha256-pqPrcGA1atBR4yH6/QHdxbSQ5R9X4nSyZtZ83q6jAvQ=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css><script src=https://cdn.jsdelivr.net/npm/glightbox/dist/js/glightbox.min.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch2/>Epoch 2: Micrograd, Part 2</a></li></ul></nav></header><main><h1>Epoch 2: Micrograd, Part 2</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-completing-operations>Content: Completing Operations</a></li><li><a href=#content-non-linearity>Content: Non-Linearity</a></li><li><a href=#content-lets-make-the-graph>Content: Let&rsquo;s make the graph</a></li><li><a href=#content-neuron>Content: Neuron</a></li><li><a href=#content-pytorch>Content: PyTorch</a></li><li><a href=#content-neuron-in-pytorch>Content: Neuron in PyTorch</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi! I&rsquo;m Mon, this is Epoch 2, Micrograd, Part 2, in the previous epoch we learnt how to implement the <strong>addition</strong> and <strong>multiplication</strong> and build a tiny engine that can calculate <strong>local derivatives</strong> and <strong>global gradients</strong>.</p><p>Before jumping to Neural Network (nn), we should implement:</p><ol><li><strong>subtraction (errors, losses)</strong></li><li><strong>powers (squared loss)</strong></li><li><strong>division (normalization, scaling)</strong></li><li><strong>non-linear functions</strong></li></ol><h2 id=content-completing-operations>Content: Completing Operations</h2><p><strong>Subtraction</strong></p><p>$$
c = a - b
$$</p><p>rewrite:
$$
c = a + (-b)
$$</p><p>Derivatives:</p><p>$$
\frac{\partial c}{\partial a} = 1
$$</p><p>$$
\frac{\partial c}{\partial b} = -1
$$</p><p>means if b increases slightly, c decreases by the same amount</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Subtraction</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__sub__</span>(self, other):
</span></span><span style=display:flex><span>    other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> self <span style=color:#f92672>+</span> (<span style=color:#f92672>-</span>other)
</span></span></code></pre></div><p><strong>Negation</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Negation</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__neg__</span>(self):
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> Value(<span style=color:#f92672>-</span>self<span style=color:#f92672>.</span>data, (self,), <span style=color:#e6db74>&#39;neg&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_backward</span>():
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#f92672>*</span> out<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>    out<span style=color:#f92672>.</span>_backward <span style=color:#f92672>=</span> _backward
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><p><strong>Power</strong></p><p>Let:</p><p>$$
c = a^n
$$</p><p>Where:</p><ul><li><code>a</code> is a Value</li><li><code>n</code> is a constant</li></ul><p>Derivative:</p><p>$$
\frac{\partial c}{\partial a} = n \cdot a^{n-1}
$$</p><p>For here the exponent is not a <em>Value</em> to</p><ol><li>keep the engine minimal</li><li>avoid unnecessary graph complexity</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__pow__</span>(self, n):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> isinstance(n, (int, float))
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> Value(self<span style=color:#f92672>.</span>data <span style=color:#f92672>**</span> n, (self,), <span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;**</span><span style=color:#e6db74>{</span>n<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_backward</span>():
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> n <span style=color:#f92672>*</span> (self<span style=color:#f92672>.</span>data <span style=color:#f92672>**</span> (n <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>)) <span style=color:#f92672>*</span> out<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    out<span style=color:#f92672>.</span>_backward <span style=color:#f92672>=</span> _backward
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><p><strong>Division (by power)</strong></p><p>Let:</p><p>$$
\frac{a}{b} = a \cdot b^{-1}
$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__truediv__</span>(self, other):
</span></span><span style=display:flex><span>    other <span style=color:#f92672>=</span> other <span style=color:#66d9ef>if</span> isinstance(other, Value) <span style=color:#66d9ef>else</span> Value(other)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> self <span style=color:#f92672>*</span> (other <span style=color:#f92672>**</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><h2 id=content-non-linearity>Content: Non-Linearity</h2><ol><li>a linear neuron can compute: $z = w_1 x_1 + w_2 x_2 + \cdots + b$</li></ol><p>if we add more neurons to make a layer and then a network, still it will be linear.</p><p>Linear neurons are not powerful enough!<br>e.g. computing XOR needs non-linear structure.
So we need to bend the space for them! By adding a non-linear activation (e.g., <strong>tanh</strong>, <strong>sigmoid</strong>, <strong>ReLU</strong>)</p><p>Let&rsquo;s do it with <strong>tanh</strong>;</p><p>Definition:</p><p>$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$</p><p>or</p><p>$$
\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
$$</p><p>or</p><p>$$
\tanh(x) = \frac{y^2 + 1}{y^2 - 1}
$$</p><p>Range:</p><p>$$
-1 &lt; \tanh(x) &lt; 1
$$</p><p>We can use the built in tanh in python by using <code>math.tanh(x)</code> or we can go manually write the formula.</p><p>Derivative:</p><p>$$
\frac{d}{dx} \tanh(x) = 1 - \tanh(x)^2
$$</p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tanh</span>(self):
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>    t <span style=color:#f92672>=</span> (math<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>x) <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>/</span> (math<span style=color:#f92672>.</span>exp(<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>x) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)  <span style=color:#75715e># careful with parentheses</span>
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> Value(t, (self,), <span style=color:#e6db74>&#39;tanh&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_backward</span>():
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>grad <span style=color:#f92672>+=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> t <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>) <span style=color:#f92672>*</span> out<span style=color:#f92672>.</span>grad  <span style=color:#75715e># derivative</span>
</span></span><span style=display:flex><span>    out<span style=color:#f92672>.</span>_backward <span style=color:#f92672>=</span> _backward
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><h2 id=content-lets-make-the-graph>Content: Let&rsquo;s make the graph</h2><p><strong>1. Install the graph in the notebook</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install graphviz
</span></span></code></pre></div><p>and put <code>from graphviz import Digraph</code> in the beginning of the code</p><p><strong>2. Code snippet</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>trace</span>(root):
</span></span><span style=display:flex><span>    nodes, edges <span style=color:#f92672>=</span> set(), set()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build</span>(v):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> v <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> nodes:
</span></span><span style=display:flex><span>            nodes<span style=color:#f92672>.</span>add(v)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> v<span style=color:#f92672>.</span>_prev:
</span></span><span style=display:flex><span>                edges<span style=color:#f92672>.</span>add((child, v))
</span></span><span style=display:flex><span>                build(child)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    build(root)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> nodes, edges
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>draw_dot</span>(root):
</span></span><span style=display:flex><span>    dot <span style=color:#f92672>=</span> Digraph(format<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;svg&#34;</span>, graph_attr<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;rankdir&#34;</span>: <span style=color:#e6db74>&#34;LR&#34;</span>})
</span></span><span style=display:flex><span>    nodes, edges <span style=color:#f92672>=</span> trace(root)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> nodes:
</span></span><span style=display:flex><span>        uid <span style=color:#f92672>=</span> str(id(n))
</span></span><span style=display:flex><span>        <span style=color:#75715e># Create the 3-field rectangle</span>
</span></span><span style=display:flex><span>        var_name <span style=color:#f92672>=</span> n<span style=color:#f92672>.</span>name <span style=color:#66d9ef>if</span> n<span style=color:#f92672>.</span>name <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;&#39;</span>
</span></span><span style=display:flex><span>        label <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>{{</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>var_name<span style=color:#e6db74>}</span><span style=color:#e6db74> | data=</span><span style=color:#e6db74>{</span>n<span style=color:#f92672>.</span>data<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> | grad=</span><span style=color:#e6db74>{</span>n<span style=color:#f92672>.</span>grad<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> </span><span style=color:#ae81ff>}}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        dot<span style=color:#f92672>.</span>node(name<span style=color:#f92672>=</span>uid, label<span style=color:#f92672>=</span>label, shape<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;record&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># If there&#39;s an operation, create a circle for it</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> n<span style=color:#f92672>.</span>_op:
</span></span><span style=display:flex><span>            op_uid <span style=color:#f92672>=</span> uid <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;_op&#34;</span>
</span></span><span style=display:flex><span>            dot<span style=color:#f92672>.</span>node(name<span style=color:#f92672>=</span>op_uid, label<span style=color:#f92672>=</span>n<span style=color:#f92672>.</span>_op, shape<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;circle&#39;</span>)
</span></span><span style=display:flex><span>            <span style=color:#75715e># Connect operation circle to this value rectangle</span>
</span></span><span style=display:flex><span>            dot<span style=color:#f92672>.</span>edge(op_uid, uid)
</span></span><span style=display:flex><span>            <span style=color:#75715e># Connect operands (children) to the operation circle</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> child <span style=color:#f92672>in</span> n<span style=color:#f92672>.</span>_prev:
</span></span><span style=display:flex><span>                dot<span style=color:#f92672>.</span>edge(str(id(child)), op_uid)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dot
</span></span></code></pre></div><p><strong>3. Make the graph</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Inputs</span>
</span></span><span style=display:flex><span>a <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>0.5001</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;a&#39;</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>0.71</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;b&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Compute intermediates</span>
</span></span><span style=display:flex><span>t1 <span style=color:#f92672>=</span> a <span style=color:#f92672>*</span> b
</span></span><span style=display:flex><span>t1<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;t1&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>t2 <span style=color:#f92672>=</span> t1 <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>t2<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;t2&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> t2 <span style=color:#f92672>+</span> a
</span></span><span style=display:flex><span>c<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;c&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>d <span style=color:#f92672>=</span> c<span style=color:#f92672>.</span>tanh()
</span></span><span style=display:flex><span>d<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;d&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Forward pass</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Forward pass:&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;a:&#34;</span>, a<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;b:&#34;</span>, b<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;c:&#34;</span>, c<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;d:&#34;</span>, d<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Backward pass</span>
</span></span><span style=display:flex><span>d<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Backward pass (gradients):&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;a.grad:&#34;</span>, a<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;b.grad:&#34;</span>, b<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;c.grad:&#34;</span>, c<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;d.grad:&#34;</span>, d<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Draw graph</span>
</span></span><span style=display:flex><span>draw_dot(d)
</span></span></code></pre></div><p><strong>Result</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#Forward pass:</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a: 0.5001</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#b: 0.71</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#c: 1.5651709999999999</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#d: 0.9162542319535778</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#Backward pass (gradients):</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a.grad: 0.27441769195044236</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#b.grad: 0.24073332145898163</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#c.grad: 0.16047818242715928</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#d.grad: 1.0</span>
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch2-graph.png alt=epoch2-graph></p><h2 id=content-neuron>Content: Neuron</h2><p>Neuron(in computer) is a learnable unit. And as we said, to build a capable neuron, we need to activate it with a non-linearity such as tanh.</p><p>$$
y = \tanh(w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b)
$$</p><ol><li><p>Takes some inputs ($x_1$, $x_2$, &mldr;, $x_n$).</p></li><li><p>Multiplies each by a weight ($w_1$, $w_2$, &mldr;, $w_n$), which is learnable.</p></li><li><p>Adds a bias $b$ (also learnable).</p></li><li><p>Applies a non-linearity (like tanh) to produce output $y$.</p></li></ol><p><strong>Example</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x1 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>0.5</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x1&#39;</span>)
</span></span><span style=display:flex><span>x2 <span style=color:#f92672>=</span> Value(<span style=color:#ae81ff>0.7</span>, name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;x2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Weights</span>
</span></span><span style=display:flex><span>w1 <span style=color:#f92672>=</span> Value(random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;w1&#39;</span>)
</span></span><span style=display:flex><span>w2 <span style=color:#f92672>=</span> Value(random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;w2&#39;</span>)
</span></span><span style=display:flex><span>b  <span style=color:#f92672>=</span> Value(random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), name<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;b&#39;</span>)  <span style=color:#75715e># bias</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x1w1 <span style=color:#f92672>=</span> x1 <span style=color:#f92672>*</span> w1
</span></span><span style=display:flex><span>x1w1<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;x1*w1&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x2w2 <span style=color:#f92672>=</span> x2 <span style=color:#f92672>*</span> w2
</span></span><span style=display:flex><span>x2w2<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;x2*w2&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x1w1_x2w2 <span style=color:#f92672>=</span> x1w1 <span style=color:#f92672>+</span> x2w2
</span></span><span style=display:flex><span>x1w1_x2w2<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;x1*w1 + x2*w2&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x1w1_x2w2_b <span style=color:#f92672>=</span> x1w1_x2w2 <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>x1w1_x2w2_b<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;x1*w1 + x2*w2 + b&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> x1w1_x2w2_b<span style=color:#f92672>.</span>tanh()
</span></span><span style=display:flex><span>out<span style=color:#f92672>.</span>name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;tanh(x1*w1 + x2*w2 + b)&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Forward pass:&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> [x1, x2, w1, w2, b, x1w1, x2w2, x1w1_x2w2, x1w1_x2w2_b, out]:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>v<span style=color:#f92672>.</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>v<span style=color:#f92672>.</span>data<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>out<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Backward pass (gradients):&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> v <span style=color:#f92672>in</span> [x1, x2, w1, w2, b, x1w1, x2w2, x1w1_x2w2, x1w1_x2w2_b, out]:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>v<span style=color:#f92672>.</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>.grad: </span><span style=color:#e6db74>{</span>v<span style=color:#f92672>.</span>grad<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>draw_dot(out)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Forward pass:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1: 0.5000</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x2: 0.7000</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w1: -0.8194</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w2: 0.9692</span>
</span></span><span style=display:flex><span><span style=color:#75715e># b: 0.4034</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1: -0.4097</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x2*w2: 0.6784</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1 + x2*w2: 0.2687</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1 + x2*w2 + b: 0.6721</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tanh(x1*w1 + x2*w2 + b): 0.5864</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Backward pass (gradients):</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1.grad: -0.5377</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x2.grad: 0.6359</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w1.grad: 0.3281</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w2.grad: 0.4593</span>
</span></span><span style=display:flex><span><span style=color:#75715e># b.grad: 0.6562</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1.grad: 0.6562</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x2*w2.grad: 0.6562</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1 + x2*w2.grad: 0.6562</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1*w1 + x2*w2 + b.grad: 0.6562</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tanh(x1*w1 + x2*w2 + b).grad: 1.0000</span>
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch2-graph-tanh.png alt=epoch2-graph-tanh></p><h2 id=content-pytorch>Content: PyTorch</h2><p>Congrats! we built a neuron, by</p><p><strong>1. Define Value objects to hold numbers and track their parents.</strong></p><p><strong>2. Compute gradients manually with backward() using the chain rule.</strong></p><p>You can access the code here:
<a href=https://github.com/auroramonet/memo/blob/main/codes/micrograd_1.ipynb>https://github.com/auroramonet/memo/blob/main/codes/micrograd_1.ipynb</a></p><p>Let&rsquo;s rewrite in PyTorch, a standard Python library for machine learning, which can create an autograd, by using <strong>Tensors</strong> (multi-dimensional array, similar to matrix)</p><p>Tensors can store numbers (inputs, outputs, weights, bias), so <strong>autograd</strong> (Automatic differentiation engine) can perform operations on them such as computing gradients, graphs.</p><p>Pytorch has more features:</p><p><strong>Neural network modules</strong>: Predefined layers (like Linear, Conv2d) to easily build networks.</p><p><strong>Optimizers</strong>: Prebuilt algorithms like SGD or Adam to update weights automatically.</p><p><strong>GPU support</strong>: Tensors and computations can run on GPU for fast training.</p><p>Let&rsquo;s begin</p><p>Note: Python uses float64 datatype (double precision) for floats, but PyTorch uses float32 (single precision), so we convert by <code>.double()</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install torch
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>2.0</span>])<span style=color:#f92672>.</span>double()
</span></span></code></pre></div><p>Note: Tensor doesn&rsquo;t automatically store gradients of leaf nodes, to save memory and be efficient, while we can explicitly say to store which gradients for our purpose.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.5000</span>])<span style=color:#f92672>.</span>double()
</span></span><span style=display:flex><span>x2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.7000</span>])<span style=color:#f92672>.</span>double()
</span></span><span style=display:flex><span>w1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.0259</span>])<span style=color:#f92672>.</span>double()
</span></span><span style=display:flex><span>w2 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>0.1967</span>])<span style=color:#f92672>.</span>double()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>x1<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>x2<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>w1<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>w2<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Bias</span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#f92672>-</span><span style=color:#ae81ff>0.7261</span>])<span style=color:#f92672>.</span>double()
</span></span><span style=display:flex><span>b<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Forward pass</span>
</span></span><span style=display:flex><span>x1w1 <span style=color:#f92672>=</span> x1 <span style=color:#f92672>*</span> w1
</span></span><span style=display:flex><span>x2w2 <span style=color:#f92672>=</span> x2 <span style=color:#f92672>*</span> w2
</span></span><span style=display:flex><span>sum_ <span style=color:#f92672>=</span> x1w1 <span style=color:#f92672>+</span> x2w2
</span></span><span style=display:flex><span>sum_b <span style=color:#f92672>=</span> sum_ <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>out <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(sum_b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(out<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>out<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;x1.grad:&#34;</span>, x1<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;x2.grad:&#34;</span>, x2<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;w1.grad:&#34;</span>, w1<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;w2.grad:&#34;</span>, w2<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;b.grad:&#34;</span>, b<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>item())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># -0.5193578553128438</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x1.grad: 0.0189139266452323</span>
</span></span><span style=display:flex><span><span style=color:#75715e># x2.grad: 0.14364360605503948</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w1.grad: 0.3651337090624216</span>
</span></span><span style=display:flex><span><span style=color:#75715e># w2.grad: 0.5111871839819242</span>
</span></span><span style=display:flex><span><span style=color:#75715e># b.grad: 0.7302674055099487</span>
</span></span></code></pre></div><h2 id=content-neuron-in-pytorch>Content: Neuron in PyTorch</h2><iframe width=560 height=315 src=https://www.youtube.com/embed/joA6fEAbAQc title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><p>Watch this video first, it shows how a <strong>neural network</strong> works,</p><p><em><strong>nn</strong></em> is a guessing machine! You feed it with <strong>input</strong> and <strong>output</strong>, then it tries to guess the way it can reach from the input to the output, by using layers of neurons, it does this by repeating the process, and calculating the loss to learn the path with the optimum results (lower loss) by</p><p>$$
\theta \leftarrow \theta - \eta \cdot \frac{\partial L}{\partial \theta}
$$</p><p>$$
\theta_{n+1} = \theta_n - \eta \cdot \frac{\partial L}{\partial \theta}
$$</p><p>$$
\text{w.data} -= \text{lr} \cdot \text{w.grad}
$$</p><p><strong>θ (theta)</strong></p><p>A parameter</p><p>A number the model can change</p><p>e.g.
a weight
a bias</p><p><strong>← (left arrow)</strong></p><p>Means &ldquo;update / assign&rdquo;</p><p>Not an equality</p><p>Read as:</p><p>&ldquo;Replace θ with the value on the right&rdquo;</p><p><strong>− (minus sign)</strong></p><p>Means move in the opposite direction</p><p>Used because we want to reduce the loss</p><p><strong>η (eta)</strong></p><p>The learning rate</p><p>A small positive number</p><p>Controls how big the step is</p><p><em><strong>Q</strong></em> Why do we need more than 1 neuron?</p><p><em><strong>A</strong></em> Because each Neuron can only guess a formula, a simple shape (e.g. a line, a curve)</p><p><strong>neuron:</strong></p><p>$$
\text{neuron}(x) = \phi\left(\sum_{i} w_i x_i + b\right)
$$</p><p>Where:</p><p>$x_i$ → inputs</p><p>$w_i$ → weights</p><p>$b$ → bias</p><p>$\phi$ → activation function (tanh, ReLU, etc.)</p><p><img src=https://auroramonet.github.io/memo/img/epoch2-nn.jpg alt=epoch2-nn></p><p>in each layer:</p><p>all neurons have:</p><p><strong>same formula</strong></p><p><strong>same activation function</strong></p><p><strong>same number of inputs</strong></p><p><strong>same code</strong></p><p><em>But</em></p><p>they have different <strong>weights</strong> and <strong>biases</strong></p><p><em><strong>Bias</strong></em> shifts where the neuron activates.</p><p>Let&rsquo;s make a neuron in PyTorch,</p><p><strong>To be Continued in Epoch 3</strong></p><h2 id=resources>Resources</h2><h3 id=micrograd---a-tiny-autograd-engine>Micrograd - A Tiny Autograd Engine</h3><p><a href=https://github.com/karpathy/micrograd>GitHub: karpathy/micrograd</a></p><p>A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API, created by Andrej Karpathy.</p><h3 id=the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd>The spelled-out intro to neural networks and backpropagation: building micrograd</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/VMj-3S1tku0 title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2025-12-27>2025-12-27&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("main article img, main .content img, main img");e.forEach(function(e){if(e.parentElement.tagName==="A")return;const t=document.createElement("a");t.href=e.src,t.setAttribute("data-glightbox",""),t.setAttribute("data-gallery","gallery"),e.alt&&t.setAttribute("data-title",e.alt),e.parentNode.insertBefore(t,e),t.appendChild(e)});const n=GLightbox({selector:"a[data-glightbox]",touchNavigation:!0,loop:!0,autoplayVideos:!1}),t=document.querySelectorAll('code.language-python-copy, code[class*="python-copy"], code[data-lang="python-copy"]');t.forEach(function(e){let o=e.textContent||e.innerText;const n=e.closest("pre");if(!n)return;const i=n.parentElement;if(!i)return;n.style.display="none";const s=document.createElement("div");s.className="code-copy-container";const t=document.createElement("button");t.className="code-copy-button",t.textContent="Copy Code",t.setAttribute("aria-label","Copy code to clipboard"),t.addEventListener("click",function(){if(navigator.clipboard&&navigator.clipboard.writeText)navigator.clipboard.writeText(o).then(function(){t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}).catch(function(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)});else{const e=document.createElement("textarea");e.value=o,e.style.position="fixed",e.style.opacity="0",document.body.appendChild(e),e.select();try{document.execCommand("copy"),t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}catch(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)}document.body.removeChild(e)}}),s.appendChild(t),i.insertBefore(s,n)})})</script></body></html>
<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 4: Makemore, Part 1 | Memo Box</title><meta property="og:title" content="Epoch 4: Makemore, Part 1"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch4/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch4/>Epoch 4: Makemore, Part 1</a></li></ul></nav></header><main><h1>Epoch 4: Makemore, Part 1</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-1>Content: Bigram Part 1</a></li><li><a href=#content-bigram-part-2>Content: Bigram Part 2</a></li><li><a href=#content-bigram-part-3-sampling-probability-distribution>Content: Bigram Part 3: Sampling, probability distribution</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, and here is Epoch 4, Makemore Part 1.<br><br>Makemore is a character-level language model, where each character is a token.
The model is trained on a dataset of names and learns to predict the next token (character), allowing it to generate new names that sound realistic.<br><br></p><p><strong>Step 1: Setup</strong>:
Setup the virtual environment, and run the jupyter notebook<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir makemore_tutorial
</span></span><span style=display:flex><span>cd makemore_tutorial
</span></span><span style=display:flex><span>python3 -m venv venv
</span></span><span style=display:flex><span>source venv/bin/activate
</span></span><span style=display:flex><span>pip install jupyter
</span></span><span style=display:flex><span>jupyter notebook
</span></span></code></pre></div><br><p>And make a new notebook: <em>makemore</em><br></p><p><strong>Clone</strong> the <a href=https://github.com/karpathy/makemore>Karpathy&rsquo;s Makemore repo</a></p><p>Copy &ldquo;names.txt&rdquo; to the <em>makemore_tutorial</em> directory.<br><br></p><h2 class=no-counter id=content-bigram-part-1>Content: Bigram Part 1</h2><p>Let&rsquo;s begin with making a bigram<br></p><p><strong>Bigram</strong>: Given the previous character, what is the next character likely to be?</p><p><em>import the numbers list</em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>words <span style=color:#f92672>=</span> open(<span style=color:#e6db74>&#39;names.txt&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>)<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>splitlines()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>words[:<span style=color:#ae81ff>10</span>] <span style=color:#75715e>#check the words</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#e6db74>&#39;emma&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;olivia&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;ava&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;isabella&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;sophia&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;charlotte&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;mia&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;amelia&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;harper&#39;</span>,
</span></span><span style=display:flex><span> <span style=color:#e6db74>&#39;evelyn&#39;</span>]
</span></span></code></pre></div><p>We need to find a way to store all the sequence of characters&rsquo; transitions, one input, one output, for all names,<br>i.e. &ldquo;e m&rdquo;, &ldquo;m m&rdquo;, &ldquo;m a&rdquo; &mldr;</p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(w, w[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        print(ch1, ch2)
</span></span></code></pre></div><p>e.g. emma -> zip(&ldquo;emma&rdquo;,&ldquo;mma&rdquo;)<br>(&rsquo;e&rsquo;,&rsquo;m&rsquo;), (&rsquo;m&rsquo;,&rsquo;m&rsquo;), (&rsquo;m&rsquo;,&lsquo;a&rsquo;)<br>inner loop 1: e m<br>inner lopp 2: m m<br>inner loop 3: m a<br><br>Let&rsquo;s add starting and ending characters to all names, so we can add find the sequence of starting and ending with characters too.<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>     chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>     <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        print(ch1, ch2)
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># &lt;S&gt; e</span>
</span></span><span style=display:flex><span><span style=color:#75715e># e m</span>
</span></span><span style=display:flex><span><span style=color:#75715e># m m</span>
</span></span><span style=display:flex><span><span style=color:#75715e># m a</span>
</span></span><span style=display:flex><span><span style=color:#75715e># a &lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &lt;S&gt; o</span>
</span></span></code></pre></div><p>Now we need to count the bigram of characters, to do so, we make an empty dictionary. While we loop, we check if the bigram exists, if yes return the count + 1 for that bigram, if not, store the bigram with 0<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>d <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        bigram <span style=color:#f92672>=</span> (ch1, ch2)
</span></span><span style=display:flex><span>        d[bigram] <span style=color:#f92672>=</span> d<span style=color:#f92672>.</span>get(bigram, <span style=color:#ae81ff>0</span>) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>d
</span></span><span style=display:flex><span><span style=color:#75715e>#{(&#39;&lt;S&gt;&#39;, &#39;e&#39;): 1531,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (&#39;e&#39;, &#39;m&#39;): 769,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (&#39;m&#39;, &#39;m&#39;): 168,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (&#39;m&#39;, &#39;a&#39;): 2590,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (&#39;a&#39;, &#39;&lt;E&gt;&#39;): 6640,</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#...</span>
</span></span></code></pre></div><p>Now sort by</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sorted(d<span style=color:#f92672>.</span>items(), key <span style=color:#f92672>=</span> <span style=color:#66d9ef>lambda</span> kv: <span style=color:#f92672>-</span>kv[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#[((&#39;n&#39;, &#39;&lt;E&gt;&#39;), 6763),</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ((&#39;a&#39;, &#39;&lt;E&gt;&#39;), 6640),</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ((&#39;a&#39;, &#39;n&#39;), 5438),</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ((&#39;&lt;S&gt;&#39;, &#39;a&#39;), 4410),</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#...</span>
</span></span></code></pre></div><blockquote><p><em>Note 1</em>:<br>d.items() converts dictionary d into a sequence of (key, value) tuples, so each dictionary entry k1: v1 becomes the tuple (k1, v1)</p></blockquote><br><br><h2 class=no-counter id=content-bigram-part-2>Content: Bigram Part 2</h2><p>Let&rsquo;s use tensors for our dictionary<br>28 row in 28 column, think like multiplication table, to store all the bigram combinations in 1 tensor</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>N <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>int32)
</span></span></code></pre></div><p><em>26 english letters + 1 begin + 1 end</em></p><br><p>Now we store all the characters by:<br><em>1. join all words together</em><br><em>2. use set() to remove the duplicate characters, so we get all the 26 characters</em><br><em>3. use list() to make a list</em><br><em>4. sort characters with sorted()</em><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>allwords <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(words)
</span></span><span style=display:flex><span>chars <span style=color:#f92672>=</span> sorted(list(set(allwords)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>charts
</span></span><span style=display:flex><span><span style=color:#75715e># [&#39;a&#39;,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;b&#39;,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;c&#39;,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;d&#39;,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;e&#39;,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ]</span>
</span></span></code></pre></div><p>Now let&rsquo;s give characters, ordered number, which represents their location in tensor.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>stoi <span style=color:#f92672>=</span> {s:i <span style=color:#66d9ef>for</span> i,s <span style=color:#f92672>in</span> enumerate(chars)}
</span></span><span style=display:flex><span>stoi[<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>26</span>
</span></span><span style=display:flex><span>stoi[<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>stoi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># {&#39;a&#39;: 0,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;b&#39;: 1,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;c&#39;: 2,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># &#39;d&#39;: 3,</span>
</span></span><span style=display:flex><span><span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span><span style=color:#75715e># }</span>
</span></span></code></pre></div><br>Next is to implement the tensor mapping in our loop<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        N[ix1,ix2] <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>N
</span></span></code></pre></div><br><p>&ndash;> <strong>Make plot of the tensor</strong><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt <span style=color:#75715e>#1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline <span style=color:#75715e>#2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>16</span>,<span style=color:#ae81ff>16</span>)) <span style=color:#75715e>#4</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(N, cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Blues&#39;</span>) <span style=color:#75715e>#5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>28</span>): 
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>28</span>):
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>text(j, i, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>chars[i]<span style=color:#e6db74>}{</span>chars[j]<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>N[i,j]<span style=color:#f92672>.</span>item()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>, <span style=color:#75715e>#9</span>
</span></span><span style=display:flex><span>                 ha<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;center&#39;</span>, va<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;center&#39;</span>, fontsize<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>axis(<span style=color:#e6db74>&#39;off&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><em><strong>line 1</strong></em> imports plot lib<br><em><strong>line 2</strong></em> jupyter notebook command to say plot inside the current window<br><em><strong>line 4</strong></em> canvas size<br><br><em><strong>line 5</strong></em> imshow, shows the colormap of tensor, imshow internally does linear scalar mapping for the color map<br><br>it finds max: darkest color and min: lightest color of the numbers, then it normalize all the other numbers with:<br></p><p>$$
\text{normalized value} = \frac{N[i,j] - N_{\min}}{N_{\max} - N_{\min}}
$$</p><br><p><em><strong>line 7</strong></em> loops over all i, and j<br></p><p><em><strong>line 9</strong></em> note: x: j , y: i , it finds the cell in the tensor, then it prints the ij, and with <strong>N[i,j].item()</strong> it prints the value of the tensor</p><p><br><br><img src=https://auroramonet.github.io/memo/img/epoch4-bigram_heatmap.png alt=epoch4-bigram_heatmap></p><h2 class=no-counter id=content-bigram-part-3-sampling-probability-distribution>Content: Bigram Part 3: Sampling, probability distribution</h2><p>Now let’s create a probability distribution for each row, so we can determine the likelihood of the next character.<br>Each row represents how many times each character follows a given character.<br>For example, row 0 (corresponding to &lsquo;a&rsquo;) might show: &lsquo;aa&rsquo;: 1000, &lsquo;ab&rsquo;: …, &lsquo;ac&rsquo;: … and so on.</p><br>If we make our name generator now, we can make it deterministic character picker based on the probabilities distribution (weights), but we need to add creativity and non deterministic way of picking the next character,<br><br><p>So Instead of deterministically choosing the highest-probability character, we perform probabilistic sampling: each selection is influenced by the distribution’s weights but allows variability. Over repeated selections, the resulting frequencies approximate the original distribution, so the weights govern the process on average rather than exactly each time.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist <span style=color:#f92672>=</span> N[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>float() <span style=color:#75715e>#1</span>
</span></span><span style=display:flex><span>dist <span style=color:#f92672>/=</span> dist<span style=color:#f92672>.</span>sum() <span style=color:#75715e>#2</span>
</span></span><span style=display:flex><span>dist
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([0.0164, 0.0160, 0.0139,...])</span>
</span></span></code></pre></div><p><em><strong>line 1</strong></em> convert tensor values of row 0 to have float decimal<br><em><strong>line 2</strong></em> probability distribution: row 0 is normalized by dividing to the sum of all occurrences to get the probabilty of each values, so the sum of all probs will be 1 (dist.sum() = 1).<br></p><p><strong>Sampling</strong>: picking one outcome at random from a probability distribution.<br>As we said, to add some level of probabilty and creativity, and non deterministic results,<br>we need to have some randomness,<br>In here we use <em>manual_seed</em> so our random number can be generated the same (for testing purposes)<br><br>&ndash;> <strong>1. probability distribution</strong>
this is a simple example of probabity distribution from a simple 1D tensor, note that later we can use the probabilty distribution from the tensor we made for the names.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>112234234</span>)
</span></span><span style=display:flex><span>p <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>, generator <span style=color:#f92672>=</span> g)
</span></span><span style=display:flex><span>p <span style=color:#f92672>/=</span> p<span style=color:#f92672>.</span>sum
</span></span><span style=display:flex><span>p
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([0.3001, 0.2650, 0.4349])</span>
</span></span></code></pre></div><p>&ndash;> <strong>2. sample from prob. dist. with multinomial</strong></p><p><em><strong>input:</strong></em> prob. dist. (index + probabilities)<br></p><p><em><strong>output(each sample):</strong></em> indice, representing the occurrence of a member in prob. dist.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>multinomial(p, num_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>, replacement<span style=color:#f92672>=</span> true, generator<span style=color:#f92672>=</span> g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([1, 1, 0, 1, 0, 0, 2, 0, 0, 1, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2,...])</span>
</span></span></code></pre></div><p><em>replacement=true</em>: It is allowed to use same indices for samples<br>if it is false, the result would only have not any repated indices, and in this case the num_samples must be equal or less than the p.size(0)<br><br></p><p>&ndash;> <strong>3. use names prob. dist.</strong><br>Now lets use the prob. dist. we made <strong>dist</strong> to feed our <strong>multinomial</strong> to get one sample<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sample <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(dist, num_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sample<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span><span style=color:#75715e>#17</span>
</span></span></code></pre></div><p>Now we look for the index 17 in our tensor <strong>N</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>itos[ix]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&#39;u&#39;</span>
</span></span></code></pre></div><p><br>so &lsquo;u&rsquo; the next character if we use &lsquo;a&rsquo; -> au<br><br>Now let&rsquo;s make a loop, to make names</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>11224234</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        dist_all <span style=color:#f92672>=</span> N[ix]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>        dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(dist_all, num_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g )<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        out<span style=color:#f92672>.</span>append(itos[ix])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>27</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(out))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#iumolinevamy&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#pph&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#lw&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#rao&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tttee&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#llun&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#llylelezauerdr&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#na&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#yars&lt;E&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#ieeni&lt;E&gt;</span>
</span></span></code></pre></div><p>This loop runs 10 times (generates 10 names)<br><br>For Each name:<br>It generates a new ix everytime (sample of row), it append the character, then it uses the ix as the new multinomial initiator for the next loop,<br><br>when it hits <code>&lt;E></code> index:27,<br>( it ends this step, it prints <em><strong>out</strong></em> (name),<br>then it clears <em><strong>out</strong></em> for the new name (next batch of loops)</p><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-01>2026-01-01&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
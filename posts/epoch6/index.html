<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 6: Makemore, Part 3 | Memo Box</title><meta property="og:title" content="Epoch 6: Makemore, Part 3"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch6/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.a6a3eb7060356ad051e321fafd01ddc5b490e51f57e274b266d67cdeaea302f4.css integrity="sha256-pqPrcGA1atBR4yH6/QHdxbSQ5R9X4nSyZtZ83q6jAvQ=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/glightbox/dist/css/glightbox.min.css><script src=https://cdn.jsdelivr.net/npm/glightbox/dist/js/glightbox.min.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch6/>Epoch 6: Makemore, Part 3</a></li></ul></nav></header><main><h1>Epoch 6: Makemore, Part 3</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</a></li><li><a href=#content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</a></li><li><a href=#content-bigram-part-10-sampling>Content: Bigram Part 10: sampling</a></li><li><a href=#content-mlp-part-1>Content: MLP Part 1</a></li><li><a href=#content-mlp-part-2>Content: MLP Part 2</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, in the previous epoch we learnt how to create the forward pass, we need to create our backward pass too</p><h2 class=no-counter id=content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</h2><p>At first lets learn some definitions:<br></p><p>in <strong>Classification</strong> we predict a label such as cat vs dog.<br><br>in <strong>Regression</strong> we predict a number or value.</p><br>In each of these we use different approaches to calculate the loss<br>in our micrograd we used mean squared error, and here we use neg log probs<br><br><p>From the previous epoch the forward pass</p><p><em><strong>Step 1: init g and W</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g)
</span></span></code></pre></div><p><em><strong>Step 2: forward pass:</strong></em></p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss
</span></span></code></pre></div><br>Instead of the loop (check the end of epoch 5), let's make the loss computation more straightforward<br>we can use a better vectorized method:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><p><em>torch.arange(n,m)</em> simply creates a vector with items n to m in order.</p><h2 class=no-counter id=content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</h2><p><em><strong>Step 3: set gradient of W to 0</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span></code></pre></div><p><em><strong>Step 4: backward()</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>W<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([28, 28])</span>
</span></span></code></pre></div><p>At this point we stored the gradients of W, check:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch6-gradw.png alt=epoch6-gradw><br></p><p><em><strong>Step 5: update Weights</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><br>We can repeat the process of forward -> check loss -> backward and store the new gradients of W -> update W -> forward and repeat<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>item()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.01</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p>Until now we used one word, and did the forward backward, and training steps manually, let&rsquo;s make a loop that does the autograd many times and for all the dataset of the names,<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#init</span>
</span></span><span style=display:flex><span>xs, ys <span style=color:#f92672>=</span> [], []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    chars <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> char1, char2 <span style=color:#f92672>in</span> zip(chars, chars[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[char1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[char2]
</span></span><span style=display:flex><span>        xs<span style=color:#f92672>.</span>append(ix1)
</span></span><span style=display:flex><span>        ys<span style=color:#f92672>.</span>append(ix2)
</span></span><span style=display:flex><span>xs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(xs)
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(ys)
</span></span><span style=display:flex><span>num <span style=color:#f92672>=</span> xs<span style=color:#f92672>.</span>nelement()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#gradient descent</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>    counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>    probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(num), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward</span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update </span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.8504281044006348</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.755488872528076</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.6703531742095947</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.593259334564209</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.5231285095214844</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.4592230319976807</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.400963306427002</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.347846508026123</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.299407482147217</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2552058696746826</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2148220539093018</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.575495481491089</span>
</span></span></code></pre></div><p>The optimized loss will be around 2.5, which is the same when we calculate with the number of occurrences:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>sumlogprobs <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span><span style=color:#75715e>#for w in [&#34;monqf&#34;]:</span>
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        logprob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>log(prob)
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        neglogprob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>logprob
</span></span><span style=display:flex><span>        sumlogprobs <span style=color:#f92672>+=</span> neglogprob
</span></span><span style=display:flex><span>       <span style=color:#75715e># print(f&#39;{ch1}, {ch2}: {prob:.3f} {logprob: .4f} {neglogprob: .4f}&#39;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(n)
</span></span><span style=display:flex><span>print(sumlogprobs)
</span></span><span style=display:flex><span>print((sumlogprobs<span style=color:#f92672>/</span>n)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#228146</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(559962.3125)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.4544034004211426</span>
</span></span></code></pre></div><p>As you see, roughly, we are getting the same result with the counting and normalizing method because we are using a simple linear one layer, but later we will add more complexities, neurons and non-linearity, increase contexts(not just 2 characters bigrams) to create a better nn and lower the loss.<br><br></p><p>Note: For smoothing our dataset to prevent extremes, including infinity, we added a small number to the items of our dataset,<br></p><p>There is another way for smoothing called<br><strong>regularization loss</strong>: a penalty added to the loss that discourages extreme or overly confident weights in W.<br><br>e.g. Making W closer to 0, can make the loss more unified, so<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(num), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean() <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.01</span><span style=color:#f92672>*</span>(W<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><br><hr><h2 class=no-counter id=content-bigram-part-10-sampling>Content: Bigram Part 10: sampling</h2><p>Now let&rsquo;s pick some names as the sampling of our nn model by generating names.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#sampling</span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    name <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(torch<span style=color:#f92672>.</span>tensor([ix]), num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>        counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(probs, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        name<span style=color:#f92672>.</span>append(itos[ix])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>27</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(name))
</span></span><span style=display:flex><span>        
</span></span></code></pre></div><blockquote><p>melylin<code>&lt;E></code><br>tosh<code>&lt;E></code><br>elinen<code>&lt;E></code><br>n<code>&lt;E></code><br>gsahrmaquan<code>&lt;E></code><br></p></blockquote><p>You can access the code here:
<a href=https://github.com/auroramonet/memo/blob/main/codes/makemore1.ipynb>https://github.com/auroramonet/memo/blob/main/codes/makemore1.ipynb</a></p><hr><h2 class=no-counter id=content-mlp-part-1>Content: MLP Part 1</h2><p>Now we are getting into making a multi layer neural net,<br><br>Karpathy introduced Bengio et al. 2003 (MLP language model) paper.<br></p><p><img src=https://auroramonet.github.io/memo/img/epoch6-mlp.png alt=epoch6-mlp></p><br>The paper shows the structure of building a language model, which has 18k words,<br><p>feed 3 words and predict the 4th word.<br><br>At the bottom, we have 3 words, which they have an index in the 17k words dataset.<br><br></p><p>Then each of these words use look up table <strong>C</strong>(matrix of 17k * num of neurons or embedding dim (e.g. 30)) which corresponds to the embedding vector for that word.<br><br>for each word:<br>row: 17k<br>column: 30<br></p><br>in here the input layer has 3 words * 30 neurons = 90 neurons<br><p>In the hidden layer, we can use any size (<strong>hyper parameter</strong>), and all of the neurons are fully connected to the previous layer of neurons (90)<br><br></p><p>In the output layer, we have 17k neurons (all words), all are connected to the hidden layer, 17k logits -> softmax -> prob. dist. &ndash;> L , then sampling the 4th word index (or in training phase: fine tuning the parameters to minimize the loss for the correct 4th word)<br><br></p><p>all parameters: weights and biases of all layers</p><p><br><br>We are going to make our 3+1 chars dataset, input 3 words, output 4th words (compare with the previous bigrams input 1, output 1)</p><p><br><br></p><p>Let&rsquo;s begin with making our chars, stoi and itos:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>words <span style=color:#f92672>=</span> open(<span style=color:#e6db74>&#39;names.txt&#39;</span>, <span style=color:#e6db74>&#39;r&#39;</span>)<span style=color:#f92672>.</span>read()<span style=color:#f92672>.</span>splitlines()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>chars <span style=color:#f92672>=</span> sorted(list(set(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(words))))
</span></span><span style=display:flex><span>stoi <span style=color:#f92672>=</span> {s:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span> <span style=color:#66d9ef>for</span> i,s <span style=color:#f92672>in</span> enumerate(chars)}
</span></span><span style=display:flex><span>stoi[<span style=color:#e6db74>&#39;.&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>itos <span style=color:#f92672>=</span> {i:s <span style=color:#66d9ef>for</span> s,i <span style=color:#f92672>in</span> stoi<span style=color:#f92672>.</span>items()}
</span></span><span style=display:flex><span>print(itos)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#{1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;, 0: &#39;.&#39;}</span>
</span></span></code></pre></div><br><p>Now let&rsquo;s begin with making <strong>dataset</strong> of 3 words -> 4th word<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#dataset</span>
</span></span><span style=display:flex><span>block_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> <span style=color:#75715e>#L1</span>
</span></span><span style=display:flex><span>X, Y <span style=color:#f92672>=</span> [], [] <span style=color:#75715e>#L2</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[:<span style=color:#ae81ff>5</span>]: <span style=color:#75715e>#L3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(w)
</span></span><span style=display:flex><span>    context <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>] <span style=color:#f92672>*</span> block_size <span style=color:#75715e>#L6</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch <span style=color:#f92672>in</span> w <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.&#39;</span>: <span style=color:#75715e>#L7</span>
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> stoi[ch] <span style=color:#75715e>#L8</span>
</span></span><span style=display:flex><span>        X<span style=color:#f92672>.</span>append(context)
</span></span><span style=display:flex><span>        Y<span style=color:#f92672>.</span>append(ix)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(itos[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> context), <span style=color:#e6db74>&#39;---&gt;&#39;</span>, itos[ix])
</span></span><span style=display:flex><span>        context <span style=color:#f92672>=</span> context[<span style=color:#ae81ff>1</span>:] <span style=color:#f92672>+</span> [ix] <span style=color:#75715e>#L12</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(X)
</span></span><span style=display:flex><span>Y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(Y)
</span></span></code></pre></div><p>Line 1: <strong>Block size</strong> is the context length (how many items in each context)<br><br>Line 2: <strong>X</strong> is context(inputs)<br><strong>Y</strong> is the label (outputs, or in here the 4th char)<br><br>Line 6: Init 1st row of context with [0,0,0]<br><br>Line 7: We add &lsquo;.&rsquo; to the end of each name, and use the items of a name (characters) in the loop<br><br>in our example the first name is emma, so the first char is &rsquo;e&rsquo;<br><br>Note: in here context is cleared after each loop, but we store each context in X,<br>until now we have [0,0,0] in the context, and we append it to the X<br><br></p><p>and we append the char, here &rsquo;e&rsquo; to <strong>Y</strong>, so the first element of Y is &rsquo;e&rsquo;<br></p><p>We created our first X and first Y<br><br></p><p>Line 12: <strong>Sliding Window</strong> to create the next context: We omit the oldest char and add the next char ix<br><br><br>Now our new context is [0,0,5], which in the next loop will be added as the next row of X<br>X will be something like:<br><br>[0,0,0],<br>[0,0,5],<br><br>and Y will be like:
[5,13]</p><h2 class=no-counter id=content-mlp-part-2>Content: MLP Part 2</h2><p>Now let&rsquo;s create the lookup table <strong>C</strong> with starting embedding dimension of 2 for each word<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>C <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>27</span>, <span style=color:#ae81ff>2</span>))
</span></span></code></pre></div><br>We can look up index of a tensor e.g.:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>C[<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([0.8668, 0.2768])</span>
</span></span></code></pre></div><br>and also we can look up more than one rows. e.g.<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>C[[<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>1</span>]]
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([[ 0.8668,  0.2768],</span>
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0414</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0714</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0414</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0714</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.0414</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0714</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.5640</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1473</span>]])
</span></span></code></pre></div><p><strong>Embedding Lookup</strong>:
<em>C[X]</em>: We are gathering rows from C according to indices in X<br></p><p>C.shape = (a, b)<br>X.shape = (x, y)<br>C[X].shape (x,y,b)<br></p><p>This means x tables of [y,b] size<br><br></p><p><strong>Example</strong>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># C.shape: (27, 2)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># X.shape: (3,3)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>  [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>  [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>],
</span></span><span style=display:flex><span>  [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>C[X] <span style=color:#f92672>=</span> 
</span></span><span style=display:flex><span>[
</span></span><span style=display:flex><span>  [C[<span style=color:#ae81ff>0</span>], C[<span style=color:#ae81ff>0</span>], C[<span style=color:#ae81ff>1</span>]],
</span></span><span style=display:flex><span>  [C[<span style=color:#ae81ff>0</span>], C[<span style=color:#ae81ff>1</span>], C[<span style=color:#ae81ff>2</span>]],
</span></span><span style=display:flex><span>  [C[<span style=color:#ae81ff>0</span>], C[<span style=color:#ae81ff>0</span>], C[<span style=color:#ae81ff>1</span>]],
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>C[X][<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span><span style=color:#75715e>#[C[0], C[0], C[1]]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>C[X][<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span><span style=color:#75715e>#C[1]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>C[X][<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span><span style=color:#75715e>#the first item of C[1]</span>
</span></span></code></pre></div><blockquote><p>Note: You can see the column of <em>C</em> as the dimension of input <em>X</em>, which it means each input X now has &ldquo;column of <em>C</em>&rdquo; times dimension.<br><br></p></blockquote><p>Let&rsquo;s call C[X]: <strong>emb</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>emb <span style=color:#f92672>=</span> C[X]
</span></span><span style=display:flex><span>emb<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([32, 3, 2])</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2: dim</span>
</span></span></code></pre></div><br><p><em><strong>Init W and b:</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>100</span>))
</span></span><span style=display:flex><span>b1 <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>100</span>)
</span></span></code></pre></div><p>Now we need to multiply our emb with the weights and add bias like:</p><blockquote><p>emb @ W1 + b1</p></blockquote><br>But this is not working, because of the matrix multiplication rule:<blockquote><p>In (A @ B)<br>The last dimension of A must be equal with the first dimension of B<br><br></p></blockquote><p>So in here since the shape of W1 is (6,100) and the first dim is 6, the last dim of emb must be 6 to be able to do the multiplication of the matrix.<br><br>So let&rsquo;s convert [32, 3, 2] -> [32, 6]<br></p><p>To do so, we decompose the shape into 3 parts of [32, 2] then concatenate them with <strong>cat</strong> to achieve [32,6]<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>cat([emb[:, <span style=color:#ae81ff>0</span>, :]],[emb[:, <span style=color:#ae81ff>1</span>, :]],[emb[:, <span style=color:#ae81ff>2</span>, :]], <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([32, 6])</span>
</span></span></code></pre></div><p>cat rule 1: <strong>All tensors must have the same shape in every dimension except dim</strong><br>e.g. in here dim=1<br><br></p><p>Another better way to do so is using <strong>unbind</strong>:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>unbind(emb, <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><blockquote><p>This removes a tensor dim glues and convert it to lists of tensors, in here it cut the glues of dim 1<br></p></blockquote><p>now to glue them in dim 1 we use:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>torch<span style=color:#f92672>.</span>cat(torch<span style=color:#f92672>.</span>unbind(emb, <span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><br><br></p><p><strong>.view()</strong>
We can change the dimension view of a matrix simply if the multiplication of their dims are equal, e.g.<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>18</span>)
</span></span><span style=display:flex><span>m
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([0,1,2,3,4,5,6 ... 17])</span>
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([18]) 1 dim</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>m<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tensor([[[ <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>2</span>,  <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>4</span>,  <span style=color:#ae81ff>5</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[ <span style=color:#ae81ff>6</span>,  <span style=color:#ae81ff>7</span>],
</span></span><span style=display:flex><span>         [ <span style=color:#ae81ff>8</span>,  <span style=color:#ae81ff>9</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>11</span>]],
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        [[<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>13</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>15</span>],
</span></span><span style=display:flex><span>         [<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>17</span>]]])
</span></span></code></pre></div><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>tensor([[ <span style=color:#ae81ff>0</span>,  <span style=color:#ae81ff>1</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>2</span>,  <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>4</span>,  <span style=color:#ae81ff>5</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>6</span>,  <span style=color:#ae81ff>7</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>8</span>,  <span style=color:#ae81ff>9</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>11</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>13</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>14</span>, <span style=color:#ae81ff>15</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>16</span>, <span style=color:#ae81ff>17</span>]])
</span></span></code></pre></div><p>The reason for this is because in the memory, the items of a tensor, regardless of the dims, are stored flat, 1 dim, e.g:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>m<span style=color:#f92672>.</span>storage()
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>7</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>9</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>11</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>12</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>13</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>14</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>15</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>16</span>
</span></span><span style=display:flex><span> <span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>[torch<span style=color:#f92672>.</span>storage<span style=color:#f92672>.</span>TypedStorage(dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>int64, device<span style=color:#f92672>=</span>cpu) of size <span style=color:#ae81ff>18</span>]
</span></span></code></pre></div><br>So instead of unbinding and concatenating, we can use the view directly, because it is so efficient and simple<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>emb<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#verify</span>
</span></span><span style=display:flex><span>emb<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>==</span> torch<span style=color:#f92672>.</span>cat(torch<span style=color:#f92672>.</span>unbind(emb, <span style=color:#ae81ff>1</span>), <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([[True, True, True, True, True, True],</span>
</span></span><span style=display:flex><span><span style=color:#75715e># [True, True, True, True ,...</span>
</span></span></code></pre></div><br>Now that we know this method works, let's do:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>h <span style=color:#f92672>=</span> emb<span style=color:#f92672>.</span>view(<span style=color:#ae81ff>32</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1
</span></span><span style=display:flex><span>h<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([32, 100])</span>
</span></span><span style=display:flex><span><span style=color:#75715e># instead of 32 we can use </span>
</span></span><span style=display:flex><span><span style=color:#75715e># emb.shape(0)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># or</span>
</span></span><span style=display:flex><span><span style=color:#75715e># -1</span>
</span></span></code></pre></div><blockquote><p>Note: when saying -1, pytorch automatically and internally calcualtes the total number of elements, and since we gave the other dims sizes, it will calcualte the first dim size</p></blockquote><p><br><br><em><strong>Adding tanh</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>h <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tanh(emb<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>6</span>) <span style=color:#f92672>@</span> W1 <span style=color:#f92672>+</span> b1)
</span></span></code></pre></div><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-07>2026-01-07&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("main article img, main .content img, main img");e.forEach(function(e){if(e.parentElement.tagName==="A")return;const t=document.createElement("a");t.href=e.src,t.setAttribute("data-glightbox",""),t.setAttribute("data-gallery","gallery"),e.alt&&t.setAttribute("data-title",e.alt),e.parentNode.insertBefore(t,e),t.appendChild(e)});const n=GLightbox({selector:"a[data-glightbox]",touchNavigation:!0,loop:!0,autoplayVideos:!1}),t=document.querySelectorAll('code.language-python-copy, code[class*="python-copy"], code[data-lang="python-copy"]');t.forEach(function(e){let o=e.textContent||e.innerText;const n=e.closest("pre");if(!n)return;const i=n.parentElement;if(!i)return;n.style.display="none";const s=document.createElement("div");s.className="code-copy-container";const t=document.createElement("button");t.className="code-copy-button",t.textContent="Copy Code",t.setAttribute("aria-label","Copy code to clipboard"),t.addEventListener("click",function(){if(navigator.clipboard&&navigator.clipboard.writeText)navigator.clipboard.writeText(o).then(function(){t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}).catch(function(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)});else{const e=document.createElement("textarea");e.value=o,e.style.position="fixed",e.style.opacity="0",document.body.appendChild(e),e.select();try{document.execCommand("copy"),t.textContent="Copied!",t.classList.add("copied"),setTimeout(function(){t.textContent="Copy Code",t.classList.remove("copied")},2e3)}catch(e){console.error("Failed to copy code:",e),t.textContent="Failed",setTimeout(function(){t.textContent="Copy Code"},2e3)}document.body.removeChild(e)}}),s.appendChild(t),i.insertBefore(s,n)})})</script></body></html>
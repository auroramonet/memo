<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 6: Makemore, Part 3 | Memo Box</title><meta property="og:title" content="Epoch 6: Makemore, Part 3"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch6/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch6/>Epoch 6: Makemore, Part 3</a></li></ul></nav></header><main><h1>Epoch 6: Makemore, Part 3</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</a></li><li><a href=#content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, in the previous epoch we learnt how to create the forward pass, we need to create our backward pass too</p><h2 class=no-counter id=content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</h2><p>At first lets learn some definitions:<br></p><p>in <strong>Classification</strong> we predict a label such as cat vs dog.<br><br>in <strong>Regression</strong> we predict a number or value.</p><br>In each of these we use different approaches to calculate the loss<br>in our micrograd we used mean squared error, and here we use neg log probs<br><br><p>From the previous epoch the forward pass</p><p><em><strong>Step 1: init g and W</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>11224234</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g)
</span></span></code></pre></div><p><em><strong>Step 2: forward pass:</strong></em></p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss
</span></span></code></pre></div><br>Instead of the loop (check the end of epoch 5), let's make the loss computation more straightforward<br>we can use a better vectorized method:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><p><em>torch.arange(n,m)</em> simply creates a vector with items n to m in order.</p><h2 class=no-counter id=content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</h2><p><em><strong>Step 3: set gradient of W to 0</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span></code></pre></div><p><em><strong>Step 4: backward()</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>W<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([28, 28])</span>
</span></span></code></pre></div><p>At this point we stored the gradients of W, check:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch6-gradw.png alt=epoch6-gradw><br></p><p><em><strong>Step 5: update Weights</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><br>We can repeat the process of forward -> check loss -> backward and store the new gradients of W -> update W -> forward and repeat<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>item()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.01</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p>Until now we used one word, and did the forward backward, and training steps manually, let&rsquo;s make a loop that does the autograd many times and for all the dataset of the names,<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#init</span>
</span></span><span style=display:flex><span>xs, ys <span style=color:#f92672>=</span> [], []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    chars <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> char1, char2 <span style=color:#f92672>in</span> zip(chars, chars[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[char1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[char2]
</span></span><span style=display:flex><span>        xs<span style=color:#f92672>.</span>append(ix1)
</span></span><span style=display:flex><span>        ys<span style=color:#f92672>.</span>append(ix2)
</span></span><span style=display:flex><span>xs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(xs)
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(ys)
</span></span><span style=display:flex><span>num <span style=color:#f92672>=</span> xs<span style=color:#f92672>.</span>nelement()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>11224234</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#gradient descent</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>    counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>    probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(num), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward</span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update </span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.8504281044006348</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.755488872528076</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.6703531742095947</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.593259334564209</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.5231285095214844</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.4592230319976807</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.400963306427002</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.347846508026123</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.299407482147217</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2552058696746826</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2148220539093018</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.575495481491089</span>
</span></span></code></pre></div><p>The optimized loss will be around 2.5, which is the same when we calculate with the number of occurrences.:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>sumlogprobs <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span><span style=color:#75715e>#for w in [&#34;monqf&#34;]:</span>
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        logprob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>log(prob)
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        neglogprob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>logprob
</span></span><span style=display:flex><span>        sumlogprobs <span style=color:#f92672>+=</span> neglogprob
</span></span><span style=display:flex><span>       <span style=color:#75715e># print(f&#39;{ch1}, {ch2}: {prob:.3f} {logprob: .4f} {neglogprob: .4f}&#39;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(n)
</span></span><span style=display:flex><span>print(sumlogprobs)
</span></span><span style=display:flex><span>print((sumlogprobs<span style=color:#f92672>/</span>n)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#228146</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(559962.3125)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.4544034004211426</span>
</span></span></code></pre></div><p>As you see, roughly, we are getting the same result with the counting and normalizing method because we are using a simple linear one layer, but later we will add more complexities, neurons and non-linearity, increase contexts(not just 2 characters bigrams) to create a better nn and lower the loss.<br><br></p><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-05>2026-01-05&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
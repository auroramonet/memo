<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 6: Makemore, Part 3 | Memo Box</title><meta property="og:title" content="Epoch 6: Makemore, Part 3"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch6/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch6/>Epoch 6: Makemore, Part 3</a></li></ul></nav></header><main><h1>Epoch 6: Makemore, Part 3</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</a></li><li><a href=#content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</a></li><li><a href=#content-bigram-part-10-sampling>Content: Bigram Part 10: sampling</a></li><li><a href=#content-mlp-part-1>Content: MLP Part 1</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, in the previous epoch we learnt how to create the forward pass, we need to create our backward pass too</p><h2 class=no-counter id=content-bigram-part-8-make-forward-pass-for-bigrams>Content: Bigram Part 8: Make forward pass for bigrams</h2><p>At first lets learn some definitions:<br></p><p>in <strong>Classification</strong> we predict a label such as cat vs dog.<br><br>in <strong>Regression</strong> we predict a number or value.</p><br>In each of these we use different approaches to calculate the loss<br>in our micrograd we used mean squared error, and here we use neg log probs<br><br><p>From the previous epoch the forward pass</p><p><em><strong>Step 1: init g and W</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g)
</span></span></code></pre></div><p><em><strong>Step 2: forward pass:</strong></em></p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>loss
</span></span></code></pre></div><br>Instead of the loop (check the end of epoch 5), let's make the loss computation more straightforward<br>we can use a better vectorized method:<br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><p><em>torch.arange(n,m)</em> simply creates a vector with items n to m in order.</p><h2 class=no-counter id=content-bigram-part-9-make-backward-pass-for-bigrams>Content: Bigram Part 9: Make backward pass for bigrams</h2><p><em><strong>Step 3: set gradient of W to 0</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span></code></pre></div><p><em><strong>Step 4: backward()</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>W<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([28, 28])</span>
</span></span></code></pre></div><p>At this point we stored the gradients of W, check:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch6-gradw.png alt=epoch6-gradw><br></p><p><em><strong>Step 5: update Weights</strong></em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><br>We can repeat the process of forward -> check loss -> backward and store the new gradients of W -> update W -> forward and repeat<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>5</span>), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>item()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>loss<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.01</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span></code></pre></div><p>Until now we used one word, and did the forward backward, and training steps manually, let&rsquo;s make a loop that does the autograd many times and for all the dataset of the names,<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#init</span>
</span></span><span style=display:flex><span>xs, ys <span style=color:#f92672>=</span> [], []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span>    chars <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> char1, char2 <span style=color:#f92672>in</span> zip(chars, chars[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[char1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[char2]
</span></span><span style=display:flex><span>        xs<span style=color:#f92672>.</span>append(ix1)
</span></span><span style=display:flex><span>        ys<span style=color:#f92672>.</span>append(ix2)
</span></span><span style=display:flex><span>xs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(xs)
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(ys)
</span></span><span style=display:flex><span>num <span style=color:#f92672>=</span> xs<span style=color:#f92672>.</span>nelement()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn((<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>), generator<span style=color:#f92672>=</span>g, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#gradient descent</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e>#forward pass</span>
</span></span><span style=display:flex><span>    xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>    logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>    counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>    probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(num), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>    print(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#backward</span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#update </span>
</span></span><span style=display:flex><span>    W<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>10</span> <span style=color:#f92672>*</span> W<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.8504281044006348</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.755488872528076</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.6703531742095947</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.593259334564209</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.5231285095214844</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.4592230319976807</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.400963306427002</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.347846508026123</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.299407482147217</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2552058696746826</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#3.2148220539093018</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.575495481491089</span>
</span></span></code></pre></div><p>The optimized loss will be around 2.5, which is the same when we calculate with the number of occurrences.:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>n <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>sumlogprobs <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words:
</span></span><span style=display:flex><span><span style=color:#75715e>#for w in [&#34;monqf&#34;]:</span>
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        logprob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>log(prob)
</span></span><span style=display:flex><span>        n <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        neglogprob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>logprob
</span></span><span style=display:flex><span>        sumlogprobs <span style=color:#f92672>+=</span> neglogprob
</span></span><span style=display:flex><span>       <span style=color:#75715e># print(f&#39;{ch1}, {ch2}: {prob:.3f} {logprob: .4f} {neglogprob: .4f}&#39;)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(n)
</span></span><span style=display:flex><span>print(sumlogprobs)
</span></span><span style=display:flex><span>print((sumlogprobs<span style=color:#f92672>/</span>n)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#228146</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(559962.3125)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.4544034004211426</span>
</span></span></code></pre></div><p>As you see, roughly, we are getting the same result with the counting and normalizing method because we are using a simple linear one layer, but later we will add more complexities, neurons and non-linearity, increase contexts(not just 2 characters bigrams) to create a better nn and lower the loss.<br><br></p><p>Note: For smoothing our dataset to prevent extremes, including infinity, we added a small number to the items of our dataset,<br></p><p>There is another way for smoothing called<br><strong>regularization loss</strong>: a penalty added to the loss that discourages extreme or overly confident weights in W.<br><br>e.g. Making W closer to 0, can make the loss more unified, so<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>probs[torch<span style=color:#f92672>.</span>arange(num), ys]<span style=color:#f92672>.</span>log()<span style=color:#f92672>.</span>mean() <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.01</span><span style=color:#f92672>*</span>(W<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>mean()
</span></span></code></pre></div><br><hr><h2 class=no-counter id=content-bigram-part-10-sampling>Content: Bigram Part 10: sampling</h2><p>Now let&rsquo;s pick some names as the sampling of our nn model by generating names.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#sampling</span>
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>145324423</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    name <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(torch<span style=color:#f92672>.</span>tensor([ix]), num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>        counts <span style=color:#f92672>=</span> logits<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(probs, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g)<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        name<span style=color:#f92672>.</span>append(itos[ix])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>27</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(name))
</span></span><span style=display:flex><span>        
</span></span></code></pre></div><blockquote><p>melylin<code>&lt;E></code><br>tosh<code>&lt;E></code><br>elinen<code>&lt;E></code><br>n<code>&lt;E></code><br>gsahrmaquan<code>&lt;E></code><br></p></blockquote><p>You can access the code here:
<a href=https://github.com/auroramonet/memo/blob/main/codes/makemore1.ipynb>https://github.com/auroramonet/memo/blob/main/codes/makemore1.ipynb</a></p><hr><h2 class=no-counter id=content-mlp-part-1>Content: MLP Part 1</h2><p>Now we are getting into making a multi layer neural net,<br><br>Karpathy introduced Bengio et al. 2003 (MLP language model) paper.<br></p><p><img src=https://auroramonet.github.io/memo/img/epoch6-mlp.png alt=epoch6-mlp></p><br>The paper show the structure of building a language model, which has 18k words,<br><p>feed 3 words and predict the 4th word.<br><br>At the bottom, we have 3 words, which they have an index in the 17k words dataset.<br><br></p><p>Then each of these words use look up table <strong>C</strong>(matrix of 17k * num of neurons or embedding dim (e.g. 30)) which corresponds to embedding vector for that word.<br><br>for each word:<br>row: 17k<br>column: 30<br></p><br>in here the input layer has 3 words * 30 neurons = 90 neurons<br><p>In the hidden layer, we can use any size (<strong>hyper parameter</strong>), and all of the neurons are fully connected to the previous neurons (90)<br><br></p><p>In the output later, we have 17k neurons (all words), all are connected to the hidden layer, 17k logits -> softmax -> prob. dist. , then sampling the 4th word index (or in training phase: fine tuning the parameters to minimize the loss for the correct 4th word)<br><br></p><p>all parameters: weights and biases of all layers</p><p><br><br>We are going to make our 3+1 chars datasets, input 3 words, output 4th words (compare with the previous bigrams input 1, output 1)</p><p><br><br></p><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-05>2026-01-05&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 3: Micrograd, Part 3 | Memo Box</title><meta property="og:title" content="Epoch 3: Micrograd, Part 3"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch3/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.28b758b646b55f8499dd3a4acbd76a5e611500aa118f6843f784642628d4db6b.css integrity="sha256-KLdYtka1X4SZ3TpKy9dqXmEVAKoRj2hD94RkJijU22s=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch3/>Epoch 3: Micrograd, Part 3</a></li></ul></nav></header><main><h1>Epoch 3: Micrograd, Part 3</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#lets-train-manually-with-a-neuron>Let&rsquo;s train manually with a neuron!</a></li><li><a href=#content-nn>Content: nn</a></li><li><a href=#content-neuron>Content: Neuron</a></li><li><a href=#content-layer>Content: Layer</a></li><li><a href=#content-wires-mlp>Content: Wires (MLP)</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, This is Epoch 3, Micrograd Part 3, in the previous epoch we learnt how to create a neuron manually, and we made an introduction about Tensors and PyTorch, now we are going to learn how to make a simple neural network (nn) layers of neurons</p><h2 id=lets-train-manually-with-a-neuron>Let&rsquo;s train manually with a neuron!</h2><p>Let me go through a neuron first, an abstract about its mechanism with a practical example:</p><p>neuron is</p><p align=center>z = x1*w1 + x2*w2 + b</p><p>and we activate it with tanh(or other activators)</p><p align=center>y_hat = tanh(z)</p><p>Also for the Loss, let&rsquo;s use squared error:</p><p align=center>L = (y_hat - y_true)^2</p><p><strong>y_hat</strong> is the prediction, output that comes from our model, it changes every run with a little more trained weights and bias to get closer to <strong>y_true</strong>: the output from our dataset, predefined, and our goal to reach!</p><p><strong>Step 0</strong></p><p>Initialize inputs and random weights & bias<br><br>w1: 0.0259 (learnable)<br>w2: 0.1967 (learnable)<br>b: 0.5 (learnable)<br>x1: 0.5<br>x2: 0.7<br>y_true = 1.0<br>lr = 0.1<br></p><p><strong>Step 1: First Forward pass</strong></p><p>Calculate <em>y_hat</em> with the inputs and the current weights and bias</p><p>z = (0.5 * 0.0259) + (0.7 * 0.1967) + 0.5<br>z = 0.65064<br>y_hat = tanh(0.65064) ≈ 0.572101</p><p><em>Note:</em> tanh(x) splash a number into a<br>number range: -1 &lt; y &lt; 1</p><p><strong>Step 2: Compute Loss</strong></p><p>L = (y_hat - y_true)^2 ≈ 0.1831</p><p><em><strong>Everytime we check this loss to go lower</strong></em></p><p>Now we need to go backward, calculate the gradients of loss (L) with respect to the inputs, then create and use new weights and bias, then repeat the process of going forward, backward, train</p><p><strong>Step 3: Backward Pass: compute gradients</strong></p><p>To compute the gradients of <strong>Loss(L)</strong> with respect to the parameters we need to begin from calculating the gradient of L w.r.t its parents (y_hat and y_true),<br>then the gradient of L w.r.t <strong>tanh(y_hat)</strong><br>then keep going backward and calculate the gradient of <strong>L</strong> w.r.t its parents by multiplying <em>∂L/∂child * ∂child/∂parent</em> (Chain Rule)<br>and continue the process until we reach to the inputs.</p><p><strong>3.1: Gradient L w.r.t y_hat</strong></p><p><em>Each operation calrify what formula to use for calculating local derivatives, refer to <a href=https://auroramonet.github.io/memo/posts/epoch1/>Epoch 1</a></em></p><p>∂L/∂y_hat = 2 * (y_hat - y_true)<br>∂L/∂y_hat = -0.855798</p><p><em>note</em> Since y_true is not a leaf of other branches, no need to calcualte its gradient.</p><p><strong>3.2: Gradient y_hat w.r.t z and L w.r.t z</strong><br><br>∂y_hat/∂z = 1 - y_hat^2<br>∂y_hat/∂z ≈ 0.6727</p><p>from the Chain Rule, to calculate the gradient of L w.r.t z:</p><p>∂L/∂z = ∂L/∂y_hat * ∂y_hat/∂z<br>∂L/∂z ≈ -0.5757</p><p><strong>3.3: Gradient L w.r.t w1 and w2 and b</strong><br><br>∂L/∂w1 = ∂L/∂z * x1 ≈ -0.288<br>∂L/∂w2 = ∂L/∂z * x2 = -0.403<br>∂L/∂b = ∂L/∂z * 1 ≈ -0.5757<br></p><p><strong>3.4: Update parameters (gradient descent)</strong>
$$
\theta_{n+1} = \theta_n - \eta \cdot \frac{\partial L}{\partial \theta}
$$</p><p>w1 = w1 - lr * ∂L/∂w1 ≈ 0.0259 - 0.1 * -0.288 ≈ 0.0547<br>w2 = w2 - lr * ∂L/∂w2 ≈ 0.1967 - 0.1 * -0.403 ≈0.237<br>b = b - lr * ∂L/∂b ≈ 0.5 - 0.1 * -0.5757 ≈ 0.55757<br></p><p>As you see all numbers changed slightly! that&rsquo;s learning!<br>w1: 0.0259 -> 0.0547
w2: 0.1967 -> 0.237
b: 0.5 -> 0.55757</p><p>Now if we do the forward pass again with the new weights and bias:<br>L = (tanh(0.5 * 0.0547 + 0.7 * 0.237 + 0.55757) - 1) ^ 2<br>≈ 0.13276<br><br>0.1831 -> 0.13276
So the Loss also decreased, and going toward 0</p><p><strong>Step 4: Repeat</strong></p><h2 id=content-nn>Content: nn</h2><p>Now let&rsquo;s implement a neural network with layers in micrograd.</p><p>We learnt how actually a neuron works, and we were able to build a neuron from scratch in the micrograd, now we want to build a multi layer neurons in micrograd.</p><p>A neural network made of input and output layers + Wiring:</p><p><strong>Input layer:</strong> This layer contains input neuron, they are not computational neurons</p><p><strong>Hidden Layers:</strong> They are the computation layers, located in the middle of input and output layers.</p><p>In each hidden layer neurons follow the same formula of</p><p align=center>z = x1*w1 + x2*w2 + b</p><p>Note: in one hidden layer: neurons have same inputs but different weights each time</p><p>in hidden layer 2: neurons have different inputs than layer 1 because the inputs have changed after going through layer 1.</p><p>e.g hidden layer 1:</p><p align=center>z1 = x1*w1 + x2*w2 + b1</p><p>hidden layer 2:</p><p align=center>z2 = x3*w3 + x2*w4 + b2</p><p><strong>Wiring</strong>: Clarifies the wiring and connections between layers. We have different architecture for wiring, such as:<br><br><em><strong>MLP:</strong></em> Fully conneced neurons between layers.<br><em><strong>CNN:</strong></em> Not fully connected, uses local connections (kernels).<br><em><strong>RNN:</strong></em> Has loops, has memory<br><em><strong>Transformer:</strong></em> Use attention<br><br><br></p><p>Let&rsquo;s make nn manually wih micrograd:</p><hr><p>Firstly we begin with<br><br></p><h2 id=content-neuron>Content: Neuron</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Neuron</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, nin):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> [Value(random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>)) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nin)]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> Value(random<span style=color:#f92672>.</span>uniform(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#75715e># w * x + b</span>
</span></span><span style=display:flex><span>        act <span style=color:#f92672>=</span> sum((wi<span style=color:#f92672>*</span>xi <span style=color:#66d9ef>for</span> wi, xi <span style=color:#f92672>in</span> zip(self<span style=color:#f92672>.</span>w, x)), self<span style=color:#f92672>.</span>b)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> act<span style=color:#f92672>.</span>tanh()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parameters</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>w <span style=color:#f92672>+</span> [self<span style=color:#f92672>.</span>b]
</span></span></code></pre></div><p><strong>nin</strong> number of inputs each neuron receives<br><strong>self.w</strong> generates n random numbers between -1 and 1 for weights<br><strong>self.b</strong> generates 1 random number for bias<br><strong><strong>init</strong> of Neuron</strong> is used one time to initialize the neuron<br><strong>parameters</strong> returns a list of trainable Values in this neuron<br><strong><strong>call</strong> of Neuron</strong> forward pass<br><strong>zip</strong> pairs up ws with xs in order<br><br><br></p><h2 id=content-layer>Content: Layer</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Layer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, nin, nout):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>neurons <span style=color:#f92672>=</span> [Neuron(nin) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nout)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x):
</span></span><span style=display:flex><span>        outs <span style=color:#f92672>=</span> [n(x) <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>neurons]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> outs[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>if</span> len(outs) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> outs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parameters</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [p <span style=color:#66d9ef>for</span> neuron <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>neurons <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> neuron<span style=color:#f92672>.</span>parameters()]
</span></span></code></pre></div><p><strong>nin</strong> number of inputs each neuron receives<br><strong>nout</strong> number of neurons in this layer<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[Neuron(nin) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(nout)]
</span></span></code></pre></div><p>Creates a list of neurons<br><br>e.g. nin = 3, nout = 4<br>self.neurons = [<br>Neuron(3),<br>Neuron(3),<br>Neuron(3),<br>Neuron(3),<br>]<br><br></p><p>Until here we initialize our layer with 4 neurons (calling Layer), each neuron accepts 3 inputs (calling Neuron from Layer) in our example<br><br></p><p><strong><strong>call</strong> of Layer</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[n(x) <span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>neurons]
</span></span></code></pre></div><p>From the list of neurons we made, we give same inputs x to each neuron<br>folloiwng our previous example of nin = 3, nout = 4<br><br>imagine we add x = [x1, x2, x3]<br><br></p><p>So by this until here: we have:<br>[tanh(w11<em>x1 + w12</em>x2 + w13<em>x3 + b1),<br>tanh(w14</em>x1 + w15<em>x2 + w16</em>x3 + b2),<br>tanh(w17<em>x1 + w18</em>x2 + w19<em>x3 + b3),<br>tanh(w20</em>x1 + w21<em>x2 + w22</em>x3 + b4)]<br><br><br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>return</span> outs[<span style=color:#ae81ff>0</span>] <span style=color:#66d9ef>if</span> len(outs) <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>else</span> outs
</span></span></code></pre></div><p>This is just for the case we have 1 neuron to return 1 neuron instead of list.<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[p <span style=color:#66d9ef>for</span> neuron <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>neurons <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> neuron<span style=color:#f92672>.</span>parameters()]
</span></span></code></pre></div><p>This is a nested loop,<br>(loop over all neurons in the layer, and for each neuron it loops and extract the parameters w and b)<br><br>As a result it creates a flat array of all parameters in order<br><br>e.g<br>[
w11, w12, w13, b1,<br>w21, w22, w23, b2,<br>w31, w32, w33, b3,<br>w41, w42, w43, b4<br>]<br><br><br></p><h2 id=content-wires-mlp>Content: Wires (MLP)</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MLP</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, nin, nouts):
</span></span><span style=display:flex><span>        sz <span style=color:#f92672>=</span> [nin] <span style=color:#f92672>+</span> nouts
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> [Layer(sz[i], sz[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(nouts))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> layer(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parameters</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [p <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> layer<span style=color:#f92672>.</span>parameters()]
</span></span><span style=display:flex><span>        
</span></span></code></pre></div><p><strong>nin</strong> number of inputs to the first layer<br><strong>nouts</strong> number of neurons in each layer<br>e.g [4, 4, 1]<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sz <span style=color:#f92672>=</span> [nin] <span style=color:#f92672>+</span> nouts
</span></span></code></pre></div><p>This size of array helps us to show the inputs and outputs of each layer<br><br>e.g.<br>nin = 3<br>nouts = [4, 4, 1]<br>sz = [3, 4, 4, 1]<br><br></p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>layers <span style=color:#f92672>=</span> [Layer(sz[i], sz[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(nouts))]
</span></span></code></pre></div><p>This creates <strong>list of layers</strong>, that each layer has the property: number of inputs and number of neurons(= outputs)<br>means:<br>Layer 0: inputs = 3, outputs = 4 also Layer(3, 4)<br><br>Layer 1: inputs = 4, outputs = 4 also Layer(4, 4)<br><br>Layer 2: inputs = 4, outputs = 1 also Layer(4, 1)<br><br></p><p><em>note:</em> Layer(3, 4) will be expanded to<br>Neuron(3),<br>Neuron(3),<br>Neuron(3),<br>Neuron(3),<br></p><p><br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x):
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>       x <span style=color:#f92672>=</span> layer(x)
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> x
</span></span></code></pre></div><p>Now it&rsquo;s the time to give x to nn, because of our MLP arcitucture, we begin the loop for all layers, first layer gets x(one or more inputs), then it calculates the y which if you remember from <strong>Neuron</strong>:<br></p><p>act = sum(wi*xi for wi, xi in zip(self.w, x), self.b)<br><br>y = act.tanh()<br></p><p>because of: <em>x = layer(x)</em> , our vector y (y1 neuron 1, y2 neuron 2,&mldr;), is considered as x of the next layer<br></p><blockquote><p><strong>Important note</strong></p><p>Because we use the vector of all <strong>y</strong> values from layer <em>n</em> as the input <strong>x</strong> for layer <em>n + 1</em>,<br>Each neuron output in layer <em>n</em> can influence all the outputs of layer <em>n + k</em>.</p></blockquote><br><p><em><strong>Example:</strong></em></p><blockquote><p>MLP: nin = 3, nouts = [4,4,1]</p></blockquote><p><strong>Initial input:</strong></p><p>x = [x1, x2, x3]</p><p><strong>Layer 0: Layer(3,4)</strong></p><p>Input: [x1, x2, x3]</p><p>Neurons N1..N4 produce [y1, y2, y3, y4]</p><p>x = [y1, y2, y3, y4]</p><p><strong>Layer 1: Layer(4,4)</strong></p><p>Input: [y1, y2, y3, y4]</p><p>Neurons N5..N8 produce [y5, y6, y7, y8]</p><p>x = [y5, y6, y7, y8]</p><p><strong>Layer 2: Layer(4,1)</strong></p><p>Input: [y5, y6, y7, y8]</p><p>Neuron N9 produces single y9</p><p>x = y9</p><p><strong>Final return →</strong> y9 (Value object)<br><br><br>Excellent! now we activate non-linearity with tanh to get the <strong>y_hat</strong><br>tanh(y) = y_hat<br><br></p><p>Now we calculate Loss (L):</p><p align=center>L = (y_hat - y_true)^2</p>Then<br><br><p><strong>Update parameters (gradient descent)</strong><br>with:<br>$$
\theta_{n+1} = \theta_n - \eta \cdot \frac{\partial L}{\partial \theta}
$$<br></p><p>Let&rsquo;s implement this last step to our micrograd too<br>We want to make the initiator of MLP to calculate the tanh(y_hat), then calculate the loss, then do backward() to calculate the gradient, then find the new parameters (train), then find the new loss, and do these steps in loops k times.<br><br>I call it &ldquo;tunning a big radio with many knobs&rdquo; the better result is for being more precise (smaller learning ratio with more steps)</p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>): <span style=color:#75715e>#1</span>
</span></span><span style=display:flex><span>    ypred <span style=color:#f92672>=</span> [n(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> xs] <span style=color:#75715e>#2</span>
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> sum((y_hat <span style=color:#f92672>-</span> y_true)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#66d9ef>for</span> y_true, y_hat <span style=color:#f92672>in</span> zip(ys, ypred)) <span style=color:#75715e>#3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> n<span style=color:#f92672>.</span>parameters(): <span style=color:#75715e>#5</span>
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>grad <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span> <span style=color:#75715e>#6</span>
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>.</span>backward() <span style=color:#75715e>#7</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> n<span style=color:#f92672>.</span>parameters(): 
</span></span><span style=display:flex><span>        p<span style=color:#f92672>.</span>data <span style=color:#f92672>+=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>0.05</span> <span style=color:#f92672>*</span> p<span style=color:#f92672>.</span>grad <span style=color:#75715e>#10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    print(k, loss<span style=color:#f92672>.</span>data) <span style=color:#75715e>#12</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>2.0</span>],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>2.5</span>, <span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>1.0</span>],
</span></span><span style=display:flex><span>    [<span style=color:#f92672>-</span><span style=color:#ae81ff>2.0</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.5</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>],
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>1.0</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.0</span>, <span style=color:#ae81ff>0.5</span>],
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1.0</span> , <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>1.0</span>]
</span></span><span style=display:flex><span>n <span style=color:#f92672>=</span> MLP(<span style=color:#ae81ff>3</span>, [<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>draw_dot(loss) <span style=color:#75715e>#24</span>
</span></span></code></pre></div><p><em>Line 1</em>: This training has 10 loops (10 times going forward and backward and train)<br><br><em>Line 2</em>: Uses each x in xs list, to call n, which is<br><em>MLP(3, [4,4,1])</em><br><br><em>Line 3</em>: Calulates the sigma of Losses,<br><br><em>Line 5</em>: Gather all the parameters from the MLP (all layers, all neurons)<br><br><em>Line 6</em>: For each step (k) we need to reset the gradient, to not add gradients of each step (k)<br><br><em>Line 7</em>: backward calculates all the gradients,<br><br></p><blockquote><p>Note: We could do backward, because loss is a Value object refer to <a href=https://auroramonet.github.io/memo/posts/epoch1/>Epoch 1</a>, that from <strong>forward pass</strong>, it stored ._prev and .data,<br>and from <strong>backward pass</strong> it stores .grad<br><br></p></blockquote><p><em>Line 10</em>: After each backward, it updates the parameters with:</p><p>$$
\theta_{n+1} = \theta_n - \eta \cdot \frac{\partial L}{\partial \theta}
$$</p><p><br><br><em>Line 12</em>:</p><blockquote><p>0 2.094403631655075<br>1 2.0751115878578568<br>2 2.058205309464937<br>3 2.0417761466655073<br>4 2.024357050402771<br>5 2.0042442666008617<br>6 1.9787204164593886<br>7 1.942384309638119<br>8 1.8821567826349068<br>9 1.7582003851194667<br></p></blockquote><p><br><br><em>Line 24</em>:
generate the latest graph<br><img src=https://auroramonet.github.io/memo/img/epoch3-garph-micrograd.svg alt=epoch3-garph-micrograd.svg></p><h2 id=resources>Resources</h2><h3 id=micrograd---a-tiny-autograd-engine>Micrograd - A Tiny Autograd Engine</h3><p><a href=https://github.com/karpathy/micrograd>GitHub: karpathy/micrograd</a></p><p>A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API, created by Andrej Karpathy.</p><h3 id=the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd>The spelled-out intro to neural networks and backpropagation: building micrograd</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/VMj-3S1tku0 title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2025-12-30>2025-12-30&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
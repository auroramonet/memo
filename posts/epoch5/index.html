<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 5: Makemore, Part 2 | Memo Box</title><meta property="og:title" content="Epoch 5: Makemore, Part 2"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch5/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch5/>Epoch 5: Makemore, Part 2</a></li></ul></nav></header><main><h1>Epoch 5: Makemore, Part 2</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</a></li><li><a href=#content-bigram-part-5-quantized-quality-of-a-model>Content: Bigram Part 5: Quantized quality of a model</a></li><li><a href=#content-bigram-part-6-model-smoothing>Content: Bigram Part 6: Model Smoothing</a></li><li><a href=#content-bigram-part-7-make-learnable-nn-for-bigrams>Content: Bigram Part 7: Make learnable nn for bigrams</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, this is Epoch 5, Makemore p2</p><h2 class=no-counter id=content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</h2><p>Just a concept to know, before we begin:<br></p><p><strong>Broadcasting</strong>:<br>Broadcasting is when PyTorch automatically &ldquo;stretches&rdquo; a smaller tensor to match a larger one so element-wise operations can work without copying data.<br><br>e.g.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]])   
</span></span><span style=display:flex><span><span style=color:#75715e># A.shape: (2, 3)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>30</span>])  
</span></span><span style=display:flex><span><span style=color:#75715e># b.shape: (3,)</span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>+</span> b
</span></span></code></pre></div><br>e.g. in here it streches (reuse, not duplicate) [10, 20, 30] internally<br><br><p><em><strong>Rule 1</strong></em>: Dimension sizes must be equal, one of them is &ldquo;1&rdquo; or one of them does not exist.<br><br><em><strong>Rule 2</strong></em>: For streching the smaller vector, bring them to the right (right-aligned)<br><br>e.g. [27,27,27] + [27,27]<br>[27,27] -> [ ,27,27] -> [1,27,27] -> [27,27,27]<br></p><hr><br><p>As you may noticed in this part:</p><blockquote><p>dist_all = N[ix].float()<br>dist_all /= dist_all.sum()</p></blockquote><p>For every character of a name, we calculate the prob. dist. every time which is inefficient. Now we decide to compute prob. dist. at once, for the whole tensor<br>Before we begin to implement that, we need to know some concepts.<br><br></p><p>Let me compare 2 approaches to find the more efficient way<br></p><p><em><strong>approach 1</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N[ix]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum()
</span></span></code></pre></div><p><em><strong>1.1.</strong></em> N[ix].shape: (28,) 1D<br><em><strong>1.2.</strong></em> Not vectorized<br><em><strong>1.3.</strong></em> Loops 1 row at a time, poor scaling<br><br><br><br></p><p><em><strong>approach 2</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><em><strong>2.1</strong></em>: firstly: N.shape: (28,28)<br><em><strong>2.2</strong></em>: secondly: dist_all.sum(1)<br><br></p><p>Basically, <strong>sum(1)</strong><br>means sum of all <strong>columns</strong><br><br>row1 = column a1 + column a2 + column a3 + &mldr;<br>row2 = column b1 + column b2 + column b3 + &mldr;<br>&mldr;<br><br><strong>sum(0)</strong> means sum of all <strong>rows</strong><br><br>column1 = row a1 + row a2 + &mldr;<br>column2 = row b1 + b2 + &mldr;<br><br><br></p><blockquote><p><strong>Note</strong> The product of dist_all.sum(0) or dist_all.sum(1) will be a 1D tensor vector n (no row or column)<br>(n,)<br><br>not having 2D can be problematic, because of <em>Broadcasting</em>, so we need to explicitly store the dimension in the result, simply by adding <em><strong>keepdim=True</strong></em>,<br><br></p></blockquote><blockquote><p>so dist_all.sum(1, keepdim=True)<br>Gives us a 2D vector: (28, 1)<br><br>and dist_all.sum(0, keepdim=True)<br>Gives us a 2D vector: (1, 28)<br><br></p></blockquote><p><em><strong>2.3</strong></em>: thirdly:<br>dist_all /= dist_all.sum(1)
correctly boradcasts the 1D vector (28, 1) to (28, 28),<br><br>now each row of dist_all.sum(1) has 28 repeated number which is calculated previously,<br></p><p>then each value in our tensor (dist_all) is divided by the corresponding dist_all.sum(1)<br><br></p><p><img src=https://auroramonet.github.io/memo/img/epoch5-keepdim.png alt=epoch5-keepdim></p><p>-> <strong>The new name gen with better efficiency:</strong><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>11224234</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(dist_all[ix], num_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g )<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        out<span style=color:#f92672>.</span>append(itos[ix])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>27</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(out))
</span></span></code></pre></div><h2 class=no-counter id=content-bigram-part-5-quantized-quality-of-a-model>Content: Bigram Part 5: Quantized quality of a model</h2><p>In the previous step, we made the prob. dist. calculator, and sampling with the prob. dist.<br>in the upcoming steps we are going to make nn model<br><br>Before that, we need to create a loss function, a quantifier that shows the quality of our model when we change the parameters (in here: characters)<br><br></p><p>Let&rsquo;s begin with this:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>ch1<span style=color:#e6db74>}</span><span style=color:#e6db74>, </span><span style=color:#e6db74>{</span>ch2<span style=color:#e6db74>}</span><span style=color:#e6db74> : </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, e: 0.048</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#e, m: 0.038</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, m: 0.025</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, a: 0.390</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, o: 0.012</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, l: 0.078</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#l, i: 0.178</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, v: 0.015</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, i: 0.354</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, a: 0.138</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, a: 0.138</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, v: 0.025</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, a: 0.250</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span></code></pre></div><ol><li>This simply shows the probability of the occurrences of the word 2 following the word 1, among the set of the word1-word2s, closer to 1 means higher likelihood to happen<br><br>-> Let&rsquo;s convert these small numbers into larger indicators for convenience of calculation:<br>Similar to <strong>loss function</strong> in micrograd, we want to use positive number, approaching to 0<br></li></ol><br>Let's do neg log:<br><p align=center>$-\log(\text{prob})$ from 0 to 1</p><p><img src=https://auroramonet.github.io/memo/img/epoch5-log.png alt=epoch5-log></p><br>This is aligned well with the requirements:<br>1. lower probability (0): higher loss<br>2. higher probabilty (1): 0 loss<br><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        logprob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>log(prob)
</span></span><span style=display:flex><span>        neglogprob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>logprob
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>ch1<span style=color:#e6db74>}</span><span style=color:#e6db74>, </span><span style=color:#e6db74>{</span>ch2<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>logprob<span style=color:#e6db74>:</span><span style=color:#e6db74> .4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>neglogprob<span style=color:#e6db74>:</span><span style=color:#e6db74> .4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, e: 0.048 -3.0408  3.0408</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#e, m: 0.038 -3.2793  3.2793</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, m: 0.025 -3.6772  3.6772</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, a: 0.390 -0.9418  0.9418</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, o: 0.012 -4.3982  4.3982</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, l: 0.078 -2.5508  2.5508</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#l, i: 0.178 -1.7278  1.7278</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, v: 0.015 -4.1867  4.1867</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, i: 0.354 -1.0383  1.0383</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, a: 0.138 -1.9796  1.9796</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, a: 0.138 -1.9829  1.9829</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, v: 0.025 -3.7045  3.7045</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, a: 0.250 -1.3882  1.3882</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span></code></pre></div><br><br>Now that we know the prob. of occurrence of bigrams, we can calculate the prob. of occurrence of a name, with multiplication of prob. of bigrams<br><p>$$
P(\text{name}) = P(e \mid \langle S \rangle) \cdot P(m \mid e) \cdot P(m \mid m) \cdot P(a \mid m) \cdot P(\langle E \rangle \mid a)
$$</p><br><p align=center><strong>prob(name) = prob(bigram1) * prob(bigram2) * ...</strong></p>As you know the likelihood of n events happen is to calculate the multiplication of their prob., so prob(a) * pro(b) * ...<br><p>And since we are using <em><strong>log</strong></em>, this rule applies:<br></p><p>$$
\log(a \cdot b \cdot c \cdot \ldots) = \log(a) + \log(b) + \log(c) + \ldots
$$</p><p>Simply by defining <em>n = 0</em> and <em>sumlogprobs = 0</em> outside the loop and using them inside the inner loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>     sumlogprobs <span style=color:#f92672>+=</span> neglogprob
</span></span><span style=display:flex><span>     n <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>then if I print:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(sumlogprobs)
</span></span><span style=display:flex><span>print((sumlogprobs<span style=color:#f92672>/</span>n)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(38.7856)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.424102306365967</span>
</span></span></code></pre></div><blockquote><p>2.4241023 considered as our loss function (quality of our model)</p><p>note: We normalize the neg of log with <strong>average</strong> , it&rsquo;s just better fairer since names can get long or short.</p></blockquote><p><br><br>Now let&rsquo;s try to see the possibility of showing up bigrams of a name and loss of a name w.r.t model parameters (bigram prob. dist.)<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;mon&#34;</span>]:
</span></span><span style=display:flex><span>     <span style=color:#75715e>#same code</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, m: 0.079 -2.5354  2.5354</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, o: 0.068 -2.6875  2.6875</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, n: 0.304 -1.1911  1.1911</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#n, &lt;E&gt;: 0.369 -0.9969  0.9969</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#4</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(7.4109)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#1.8527252674102783</span>
</span></span></code></pre></div><h2 class=no-counter id=content-bigram-part-6-model-smoothing>Content: Bigram Part 6: Model Smoothing</h2><p>If we try with a rare name which has a bigram of 0% occurrence w.r.t model parameters (bigram prob. dist.),<br>because we are using <em>logarithm</em>, at point 0, the value goes to infinity, and it causes error in our loss calculation.</p><p><br><br>e.g.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;monqf&#34;</span>]:
</span></span><span style=display:flex><span>     <span style=color:#75715e>#same code</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, m: 0.079 -2.5354  2.5354</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, o: 0.068 -2.6875  2.6875</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, n: 0.304 -1.1911  1.1911</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#n, q: 0.000 -9.1230  9.1230</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#q, f: 0.000 -inf  inf</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#f, &lt;E&gt;: 0.088 -2.4259  2.4259</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#6</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(inf)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#inf</span>
</span></span></code></pre></div><p>As you see the prob. of the bigram &ldquo;q, f&rdquo; is zero.<br>Solution: Using Model Smoothing, by adding a small number to our dataset (e.g. 1)<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> (N<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>float()
</span></span></code></pre></div><blockquote><p>If we add a large amount, the bigram prob. will be more similar and unified.</p></blockquote><h2 class=no-counter id=content-bigram-part-7-make-learnable-nn-for-bigrams>Content: Bigram Part 7: Make learnable nn for bigrams</h2><p>Until here we could make a character level language model, sampling new names w.r.t model parameters (bigram prob. dist.), and evaluating the quality of the model neg log.<br><br></p><p>But we need to make it learnable with neural network,and tune the parameters by evaluating the loss<br><br></p><p><strong>Step 1: Create training set of all bigrams</strong>
If you remember from the micrograd, in a neural network, we need to have inputs and outputs. In here inputs are first characters, and putputs are second characters in bigrams.<br><br>We feed the nn with a character, and expect to receive the correct output.<br><br>Let&rsquo;s make the set of inputs and outputs</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>xs, ys <span style=color:#f92672>=</span> [],[]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[<span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        xs<span style=color:#f92672>.</span>append(ix1)
</span></span><span style=display:flex><span>        ys<span style=color:#f92672>.</span>append(ix2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(xs)
</span></span><span style=display:flex><span>ys <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(ys)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>xs
</span></span><span style=display:flex><span>tensor([<span style=color:#ae81ff>26</span>,  <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>12</span>,  <span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ys
</span></span><span style=display:flex><span>tensor([ <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>12</span>,  <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>27</span>])
</span></span></code></pre></div><blockquote><p>Loop over all characters<br>Append all characters x in xs, and append all x+1 characters to ys<br>So bigram 1: xs[0] and ys [0]: 26,4</p></blockquote><p><em><strong>But</strong></em> there is an issue here, we have the index of the charaters (integers), they are like labels, but we need numerical values for the charcters that we can do math on them in nn. This is where we use One-hot encoding<br><br></p><p><strong>One-hot encoding</strong>: Represents a character by a vector with a single 1 at its index and zeros everywhere else.<br>e.g</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>xenc <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>one_hot(xs, num_classes<span style=color:#f92672>=</span><span style=color:#ae81ff>28</span>)<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>xenc<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#torch.Size([5, 28])</span>
</span></span><span style=display:flex><span>xenc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor([[-0.1169],</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#        [ 0.8696],</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#        [ 1.4481],</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#        [ 1.4481],</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#        [ 0.4780]])</span>
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch5-onehot.png alt=epoch5-onehot></p><br>plt.imshow(xenc)<p><img src=https://auroramonet.github.io/memo/img/epoch5-onehot-plt.png alt=epoch5-onehot-plt><br></p><blockquote><p><em>Note</em> one_hot doesnt support .. dtype=torch.float32, instead we use .float()<br></p></blockquote><p>Let&rsquo;s make the other part of nn, initializing <strong>weights</strong> with random numbers<br><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>1</span>) 
</span></span><span style=display:flex><span>neuron <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>13</span>]
</span></span></code></pre></div><blockquote><p>W = torch.randn(28, 1) means 28 inputs and 1 output neuron.<br>And also each <strong>row</strong> shows the numbers related to a specific input for all neurons, and each <strong>column</strong> shows all the weights of one neuron for different inputs.<br></p></blockquote><blockquote><p><strong>@</strong> is <em><strong>matrix multiplication</strong></em>
in line 2, we are basically multiplying <strong>xenc</strong> [n, 28]
here [5,28] with [28,1]<br><br>Product Shape is [5,1]
Based on the Matrix multiplication shape rule:</p></blockquote><p>$$
\text{shape}(A) = (m, n), \quad \text{shape}(B) = (n, p), \quad A @ B \rightarrow \text{shape}(m, p)
$$</p><br><blockquote><p>note: torch.rand returns a tensor with a random number from the standard normal distribution</p></blockquote><p><br><br>By doing the matrix multiplication we feed each neuron with n inputs<br><br></p><p><br><br>We can increase the number of the neurons too:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>)
</span></span><span style=display:flex><span>neurons <span style=color:#f92672>=</span> xenc <span style=color:#f92672>@</span> W
</span></span><span style=display:flex><span>neurons<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span><span style=color:#75715e>#[5, 28]</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#5 inputs</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#28 neurons</span>
</span></span><span style=display:flex><span>neurons
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch5-neurons.png alt=epoch5-neurons></p><blockquote><p>This[5,28] vector shows firing rate of all inputs in each neuron<br></p></blockquote><blockquote><p>tensor(0.4859)</p></blockquote><p>e.g. we want to check the score of <code>&lt;S></code> and <code>a</code> bigram so<br>(xenc @ W)[0, 4] shows the score of input 0(<code>&lt;S></code>) at neuron 4 (output, &lsquo;a&rsquo;), which later can be calculated to probabilty,<br><br></p><p>Other way to look at this matrix multiplcation:<br>The first character is picker (zeroes and 1), which 1 shows the index, and the item at index in y(s) which is for the next character is weight<br>In short:</p><blockquote><p>One-hot = selects input<br>W = weights to neurons (random at first, later will be fine tuned as parameters)<br>xenc @ W = each time picks the row of W for one input and gives scores to the output.<br></p></blockquote><blockquote><p>Simply shows: score of bigram (nm) is weight of m at the index that pointed by n</p></blockquote><p>Each output can then be turned into a probability for the next character</p><blockquote><p>In other words, every input, has 28 outputs because of 28 neurons, which represents the next characters after input, and the score(comes from weights), which later on we convert them to the probs.<br></p></blockquote><p>The math works because simply in the matrix multiplication we are multiplying input to output</p><p>As you see in the image, the outputs still are not proper for our upcoming nn, (compare loss function, with the current numbers which has negative and positive numbers, so we need to convert them to some numbers positive)<br></p><p>This is where we use <em><strong>exponent</strong></em> simply because it converts any real number into a positive number<br><br></p><p><img src=https://auroramonet.github.io/memo/img/epoch5-exp-x.png alt=epoch5-exp-x></p><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>(xenc <span style=color:#f92672>@</span> W)<span style=color:#f92672>.</span>exp()
</span></span></code></pre></div><p><img src=https://auroramonet.github.io/memo/img/epoch5-exp.png alt=epoch5-exp></p><br><p>let&rsquo;s call <em>xenc @ W</em>, logits<br>just for naming convention, means raw scores before converting into probabilties<br><br>and <em>(xenc @ W).exp()</em>, counts<br>represents similar behaviour with counts of occouerences of bigrams<br><br></p><blockquote><p><em><strong>Note</strong></em><br><br>logits-> exp()-> normalizing-> prob. dist.
This layer also called <strong>&ldquo;Softmax&rdquo;</strong></p></blockquote><p>and lets create prob. dist. from counts:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>probs <span style=color:#f92672>=</span> counts <span style=color:#f92672>/</span> counts<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>probs
</span></span></code></pre></div><blockquote><p>Note: Still we have not added a <em><strong>non inearity</strong></em> such as <em>tanh()</em></p></blockquote><p><br><br></p><p>Now that we make the neurons and the weights, and forward pass, we check if we can backpropagate the operations too, which is a necessary step in our nn, and training.<br><br></p><p>logits = xenc @ W (they are just multiplication and addition)<br><br></p><p>counts = logits.exp() (we know how to calcualte the local derivative of exp)<br><br></p><p>probs = counts / counts.sum(1, keepdims=True) (division and sum)</p><p><br><br>To be Continued</p><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-04>2026-01-04&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
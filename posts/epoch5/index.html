<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 5: Makemore, Part 2 | Memo Box</title><meta property="og:title" content="Epoch 5: Makemore, Part 2"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch5/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch5/>Epoch 5: Makemore, Part 2</a></li></ul></nav></header><main><h1>Epoch 5: Makemore, Part 2</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon</p><h2 class=no-counter id=content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</h2><p>As you may noticed in this part:</p><blockquote><p>dist_all = N[ix].float()
dist_all /= dist_all.sum()</p></blockquote><p>For every character of a name, we calculate the prob. dist. every time which is inefficient. We decide to compute prob. dist. at once, for the whole tensor<br>Before we begin to implement that, we need to know some concepts.<br><br></p><p><strong>Broadcasting</strong>:<br>Broadcasting is when PyTorch automatically &ldquo;stretches&rdquo; a smaller tensor to match a larger one so element-wise operations can work without copying data.<br><br>e.g.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]])   
</span></span><span style=display:flex><span><span style=color:#75715e># A.shape: (2, 3)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>30</span>])  
</span></span><span style=display:flex><span><span style=color:#75715e># b.shape: (3,)</span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>+</span> b
</span></span></code></pre></div><p>Streches (duplicates) [10, 20, 30] internally<br>Broadcasting happens automatically when Pytorch encounters tensors of different shapes in an element-wise operation.<br><br>Let me compare 2 approaches<br></p><p><em><strong>approach 1</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N[ix]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum()
</span></span></code></pre></div><p><em><strong>1.1.</strong></em> N[ix].shape: (28,) 1D<br><em><strong>1.2.</strong></em> Not vectorized<br><em><strong>1.3.</strong></em> Loops 1 row at a time, poor scaling<br><br><br><br></p><p><em><strong>approach 2</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><em><strong>2.1</strong></em>: firstly: N.shape: (28,28)<br><em><strong>2.2</strong></em>: secondly: dist_all.sum(1)<br><br></p><p>Basically, <strong>sum(0)</strong><br>means sum of all <strong>rows</strong><br><br>row1 = column a1 + column a2 + column a3 + &mldr;<br>row2 = column b1 + column b2 + column b3 + &mldr;<br>&mldr;<br><br><strong>sum(1)</strong> means sum of all <strong>columns</strong><br><br>column1 = row a1 + row a2 + &mldr;<br>column2 = row b1 + b2 + &mldr;<br><br><br></p><blockquote><p><strong>Note</strong> The product of dist_all.sum(0) or dist_all.sum(1) will be a 1D tensor vector n (no row or column)<br>(n,)<br><br>not having 2D can be problematic, because of <em>Broadcasting</em>, so we need to explicitly store the dimension in the result, simply by adding <em><strong>keepdim=True</strong></em>,<br><br></p></blockquote><blockquote><p>so dist_all.sum(1, keepdim=True)<br>Gives us a 2D vector: (28, 1)<br><br>and dist_all.sum(0, keepdim=True)<br>Gives us a 2D vector: (1, 28)<br><br></p></blockquote><p><em><strong>2.3</strong></em>: thirdly:<br>dist_all /= dist_all.sum(1)
correctly boradcasts the 1D vector (28, 1) to (28, 28),<br><br>now each row of dist_all.sum(1) has 28 repeated number which is calculated previously,<br></p><p>then each value in our tensor (dist_all) is divided by the corresponding dist_all.sum(1)<br><br></p><p><img src=https://auroramonet.github.io/memo/img/epoch5-keepdim.png alt=epoch5-keepdim></p><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-02>2026-01-02&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
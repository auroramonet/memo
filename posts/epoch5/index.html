<!doctype html><html lang=en-US dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Epoch 5: Makemore, Part 2 | Memo Box</title><meta property="og:title" content="Epoch 5: Makemore, Part 2"><meta property="og:type" content="article"><meta property="og:url" content="https://auroramonet.github.io/memo/posts/epoch5/"><meta property="og:image" content="https://files.catbox.moe/zh3y51.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://files.catbox.moe/zh3y51.png"><link rel=icon type=image/svg+xml href=/memo/favicon.svg><link rel=apple-touch-icon href=/memo/favicon.svg><meta name=theme-color content="#1a202c"><link rel=stylesheet href=/memo/css/main.min.7fa279f525716b6c44ad4d86208dcd4c1fb7ea3ce69b8f00834bf701768f3a60.css integrity="sha256-f6J59SVxa2xErU2GII3NTB+36jzmm48Ag0v3AXaPOmA=" crossorigin=anonymous><link rel=stylesheet href=/memo/css/custom.min.844095016ac2104500a88abc547e4d8fe47e33b7b332f8fe24c1939c38d7b34d.css integrity="sha256-hECVAWrCEEUAqIq8VH5Nj+R+M7ezMvj+JMGTnDjXs00=" crossorigin=anonymous><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body><header><nav class=path-nav><ul><li>/
<a href=/memo/>Memo Box</a>
/</li><li><a href=/memo/posts/>Posts</a>
/</li><li class=current><a href=/memo/posts/epoch5/>Epoch 5: Makemore, Part 2</a></li></ul></nav></header><main><h1>Epoch 5: Makemore, Part 2</h1><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</a></li><li><a href=#content-bigram-part-5-quantized-quality-of-a-model>Content: Bigram Part 5: Quantized quality of a model</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></nav><h2 id=intro>Intro</h2><p>Hi, I&rsquo;m Mon, this is Epoch 5, Makemore p2</p><h2 class=no-counter id=content-bigram-part-4-better-efficiency>Content: Bigram Part 4: Better efficiency</h2><p>As you may noticed in this part:</p><blockquote><p>dist_all = N[ix].float()<br>dist_all /= dist_all.sum()</p></blockquote><p>For every character of a name, we calculate the prob. dist. every time which is inefficient. Now we decide to compute prob. dist. at once, for the whole tensor<br>Before we begin to implement that, we need to know some concepts.<br><br></p><p><strong>Broadcasting</strong>:<br>Broadcasting is when PyTorch automatically &ldquo;stretches&rdquo; a smaller tensor to match a larger one so element-wise operations can work without copying data.<br><br>e.g.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>], [<span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>]])   
</span></span><span style=display:flex><span><span style=color:#75715e># A.shape: (2, 3)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>30</span>])  
</span></span><span style=display:flex><span><span style=color:#75715e># b.shape: (3,)</span>
</span></span><span style=display:flex><span>A <span style=color:#f92672>+</span> b
</span></span></code></pre></div><p>Streches (reuse, not duplicate) [10, 20, 30] internally<br>Broadcasting happens automatically when Pytorch encounters tensors of different shapes in an element-wise operation.<br><br><em><strong>Rule 1</strong></em>: Dimension sizes must be equal, one of them is &ldquo;1&rdquo; or pne of them does not exist.<br><br><em><strong>Rule 2</strong></em>: For streching the smaller vector, make them right-aligned<br><br>e.g. [27,27,27] + [1,27]<br>[1,27] -> [ ,1,27] -> [1,1,27]<br></p><p><br><br>Let me compare 2 approaches<br></p><p><em><strong>approach 1</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N[ix]<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum()
</span></span></code></pre></div><p><em><strong>1.1.</strong></em> N[ix].shape: (28,) 1D<br><em><strong>1.2.</strong></em> Not vectorized<br><em><strong>1.3.</strong></em> Loops 1 row at a time, poor scaling<br><br><br><br></p><p><em><strong>approach 2</strong></em>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p><em><strong>2.1</strong></em>: firstly: N.shape: (28,28)<br><em><strong>2.2</strong></em>: secondly: dist_all.sum(1)<br><br></p><p>Basically, <strong>sum(0)</strong><br>means sum of all <strong>rows</strong><br><br>row1 = column a1 + column a2 + column a3 + &mldr;<br>row2 = column b1 + column b2 + column b3 + &mldr;<br>&mldr;<br><br><strong>sum(1)</strong> means sum of all <strong>columns</strong><br><br>column1 = row a1 + row a2 + &mldr;<br>column2 = row b1 + b2 + &mldr;<br><br><br></p><blockquote><p><strong>Note</strong> The product of dist_all.sum(0) or dist_all.sum(1) will be a 1D tensor vector n (no row or column)<br>(n,)<br><br>not having 2D can be problematic, because of <em>Broadcasting</em>, so we need to explicitly store the dimension in the result, simply by adding <em><strong>keepdim=True</strong></em>,<br><br></p></blockquote><blockquote><p>so dist_all.sum(1, keepdim=True)<br>Gives us a 2D vector: (28, 1)<br><br>and dist_all.sum(0, keepdim=True)<br>Gives us a 2D vector: (1, 28)<br><br></p></blockquote><p><em><strong>2.3</strong></em>: thirdly:<br>dist_all /= dist_all.sum(1)
correctly boradcasts the 1D vector (28, 1) to (28, 28),<br><br>now each row of dist_all.sum(1) has 28 repeated number which is calculated previously,<br></p><p>then each value in our tensor (dist_all) is divided by the corresponding dist_all.sum(1)<br><br></p><p><img src=https://auroramonet.github.io/memo/img/epoch5-keepdim.png alt=epoch5-keepdim></p><p>-> <strong>The new name gen with better efficiency:</strong><br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>g <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>Generator()<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>11224234</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> n <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    out <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    ix <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>        ix <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>multinomial(dist_all[ix], num_samples<span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>, replacement<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, generator<span style=color:#f92672>=</span>g )<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        out<span style=color:#f92672>.</span>append(itos[ix])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> ix <span style=color:#f92672>==</span> <span style=color:#ae81ff>27</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;&#39;</span><span style=color:#f92672>.</span>join(out))
</span></span></code></pre></div><h2 class=no-counter id=content-bigram-part-5-quantized-quality-of-a-model>Content: Bigram Part 5: Quantized quality of a model</h2><p>In the previous step, we made the prob. dist. calculator, and sampling with the prob. dist.<br>in the upcoming steps we are going to make nn model<br><br>Before that, we need to create a loss function, a quantifier that shows the quality of our model when we change the parameters (in here: characters)<br><br></p><p>Let&rsquo;s begin with this:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dist_all <span style=color:#f92672>=</span> N<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>dist_all <span style=color:#f92672>/=</span> dist_all<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>ch1<span style=color:#e6db74>}</span><span style=color:#e6db74>, </span><span style=color:#e6db74>{</span>ch2<span style=color:#e6db74>}</span><span style=color:#e6db74> : </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, e: 0.048</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#e, m: 0.038</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, m: 0.025</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, a: 0.390</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, o: 0.012</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, l: 0.078</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#l, i: 0.178</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, v: 0.015</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, i: 0.354</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, a: 0.138</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, a: 0.138</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, v: 0.025</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, a: 0.250</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196</span>
</span></span></code></pre></div><ol><li>This simply shows the probability of the occurrences of word 2 following word 1, among the set of word1-word2s, closer to 1 means higher likelihood to happen<br><br>-> Let&rsquo;s convert these small numbers into larger indicators for convenience of calculation:<br>Similar to <strong>loss function</strong> in micrograd, we want to use positive number, approaching to 0<br></li></ol><br>Let's do neg log:<br><p align=center><strong>-log(prob) from 0 to 1</strong></p>![epoch5-log](https://auroramonet.github.io/memo/img/epoch5-log.png)<br>This is aligned well with the requirement:<br>1. lower probability (0): higher loss<br>2. higher probabilty (1): 0 loss<br><br><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> words[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    chs <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;&lt;S&gt;&#39;</span>] <span style=color:#f92672>+</span> list(w) <span style=color:#f92672>+</span> [<span style=color:#e6db74>&#39;&lt;E&gt;&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ch1, ch2 <span style=color:#f92672>in</span> zip(chs, chs[<span style=color:#ae81ff>1</span>:]):
</span></span><span style=display:flex><span>        ix1 <span style=color:#f92672>=</span> stoi[ch1]
</span></span><span style=display:flex><span>        ix2 <span style=color:#f92672>=</span> stoi[ch2]
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> dist_all[ix1, ix2]
</span></span><span style=display:flex><span>        logprob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>log(prob)
</span></span><span style=display:flex><span>        neglogprob <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>logprob
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>ch1<span style=color:#e6db74>}</span><span style=color:#e6db74>, </span><span style=color:#e6db74>{</span>ch2<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>prob<span style=color:#e6db74>:</span><span style=color:#e6db74>.3f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>logprob<span style=color:#e6db74>:</span><span style=color:#e6db74> .4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>neglogprob<span style=color:#e6db74>:</span><span style=color:#e6db74> .4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, e: 0.048 -3.0408  3.0408</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#e, m: 0.038 -3.2793  3.2793</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, m: 0.025 -3.6772  3.6772</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, a: 0.390 -0.9418  0.9418</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, o: 0.012 -4.3982  4.3982</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, l: 0.078 -2.5508  2.5508</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#l, i: 0.178 -1.7278  1.7278</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, v: 0.015 -4.1867  4.1867</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, i: 0.354 -1.0383  1.0383</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#i, a: 0.138 -1.9796  1.9796</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, a: 0.138 -1.9829  1.9829</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, v: 0.025 -3.7045  3.7045</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#v, a: 0.250 -1.3882  1.3882</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#a, &lt;E&gt;: 0.196 -1.6299  1.6299</span>
</span></span></code></pre></div><br><br>Now that we know the prob. of occurrence of bigrams, we can calculate the prob. of occurrence of a name, with multiplication of prob. of bigrams<br><p>$$
P(\text{name}) = P(e \mid \langle S \rangle) \cdot P(m \mid e) \cdot P(m \mid m) \cdot P(a \mid m) \cdot P(\langle E \rangle \mid a)
$$</p><br><p align=center><strong>prob(name) = prob(bigram1) * prob(bigram2)</strong></p>As you know the likelihood of n events happen is to calculate the multiplication of their prob., so prob(a) * pro(b) * ...<br><p>And since we are using <em><strong>log</strong></em>, this rule applies:<br></p><p>$$
\log(a \cdot b \cdot c \cdot \ldots) = \log(a) + \log(b) + \log(c) + \ldots
$$</p><p>Simply by defining <em>n = 0</em> and <em>sumlogprobs = 0</em> outside the loop and using them inside the inner loop:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>     sumlogprobs <span style=color:#f92672>+=</span> neglogprob
</span></span><span style=display:flex><span>     n <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span></code></pre></div><p>then if I print:<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>print(sumlogprobs)
</span></span><span style=display:flex><span>print((sumlogprobs<span style=color:#f92672>/</span>n)<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(38.7856)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#2.424102306365967</span>
</span></span></code></pre></div><blockquote><p>2.4241023 considered as our loss function (quality of our model)</p><p>note: We normalize the neg of log with <strong>average</strong> , it&rsquo;s just better fairer since names can get long or short.</p></blockquote><p><br><br>Now let&rsquo;s try to see the possibility of showing up bigrams of a name and loss of a name w.r.t model parameters (bigram prob. dist.)<br></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;mon&#34;</span>]:
</span></span><span style=display:flex><span>     <span style=color:#75715e>#same code</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span>     <span style=color:#f92672>.</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#&lt;S&gt;, m: 0.079 -2.5354  2.5354</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#m, o: 0.068 -2.6875  2.6875</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#o, n: 0.304 -1.1911  1.1911</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#n, &lt;E&gt;: 0.369 -0.9969  0.9969</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#4</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#tensor(7.4109)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#1.8527252674102783</span>
</span></span></code></pre></div><h2 class=no-counter id=resources>Resources</h2><h3 id=an-autoregressive-character-level-language-model-for-making-more-things>An autoregressive character-level language model for making more things</h3><p><a href=https://github.com/karpathy/makemore>GitHub: karpathy/makemore</a></p><h3 id=the-spelled-out-intro-to-language-modeling-building-makemore>The spelled-out intro to language modeling: building makemore</h3><iframe width=560 height=315 src=https://www.youtube.com/embed/PaCmpygFfXo title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen></iframe><div class=time><time datetime=2026-01-02>2026-01-02&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=../ class=back-link>../</a></div></div></main><footer></footer></body></html>
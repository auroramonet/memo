<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Memo on Memo Box</title>
    <link>http://localhost:1313/memo/</link>
    <description>Recent content in Memo on Memo Box</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 02 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/memo/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Epoch 5: Makemore, Part 2</title>
      <link>http://localhost:1313/memo/posts/epoch5/</link>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/memo/posts/epoch5/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Hi, I&amp;rsquo;m Mon, this is Epoch 5, Makemore p2&lt;/p&gt;&#xA;&lt;h2 class=&#34;no-counter&#34; id=&#34;content-bigram-part-4-better-efficiency&#34;&gt;Content: Bigram Part 4: Better efficiency&lt;/h2&gt;&#xA;&lt;p&gt;As you may noticed in this part:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;dist_all = N[ix].float() &lt;br&gt;&#xA;dist_all /= dist_all.sum()&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;For every character of a name, we calculate the prob. dist. every time which is inefficient. Now we decide to compute prob. dist. at once, for the whole tensor&#xA;&lt;br&gt;&#xA;Before we begin to implement that, we need to know some concepts.&#xA;&lt;br&gt;&lt;br&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Epoch 4: Makemore, Part 1</title>
      <link>http://localhost:1313/memo/posts/epoch4/</link>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/memo/posts/epoch4/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Hi, I&amp;rsquo;m Mon, and here is Epoch 4, Makemore Part 1.&lt;br&gt;&lt;br&gt; Makemore is a character-level language model, where each character is a token.&#xA;The model is trained on a dataset of names and learns to predict the next token (character), allowing it to generate new names that sound realistic.&#xA;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Step 1: Setup&lt;/strong&gt;:&#xA;Setup the virtual environment, and run the jupyter notebook &lt;br&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir makemore_tutorial&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd makemore_tutorial&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python3 -m venv venv&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source venv/bin/activate&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install jupyter&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;jupyter notebook&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br&gt;&#xA;&lt;p&gt;And make a new notebook: &lt;em&gt;makemore&lt;/em&gt;&#xA;&lt;br&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Epoch 3: Micrograd, Part 3</title>
      <link>http://localhost:1313/memo/posts/epoch3/</link>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/memo/posts/epoch3/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Hi, I&amp;rsquo;m Mon, This is Epoch 3, Micrograd Part 3, in the previous epoch we learnt how to create a neuron manually, and we made an introduction about Tensors and PyTorch, now we are going to learn how to make a simple neural network (nn) layers of neurons&lt;/p&gt;&#xA;&lt;h2 id=&#34;lets-train-manually-with-a-neuron&#34;&gt;Let&amp;rsquo;s train manually with a neuron!&lt;/h2&gt;&#xA;&lt;p&gt;Let me go through a neuron first, an abstract about its mechanism with a practical example:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Epoch 2: Micrograd, Part 2</title>
      <link>http://localhost:1313/memo/posts/epoch2/</link>
      <pubDate>Sat, 27 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/memo/posts/epoch2/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Hi! I&amp;rsquo;m Mon, this is Epoch 2, Micrograd, Part 2, in the previous epoch we learnt how to implement the &lt;strong&gt;addition&lt;/strong&gt; and &lt;strong&gt;multiplication&lt;/strong&gt; and build a tiny engine that can calculate &lt;strong&gt;local derivatives&lt;/strong&gt; and &lt;strong&gt;global gradients&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Before jumping to Neural Network (nn), we should implement:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;subtraction (errors, losses)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;powers (squared loss)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;division (normalization, scaling)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;non-linear functions&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;content-completing-operations&#34;&gt;Content: Completing Operations&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Subtraction&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;$$&#xA;c = a - b&#xA;$$&lt;/p&gt;&#xA;&lt;p&gt;rewrite:&#xA;$$&#xA;c = a + (-b)&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Epoch1: Micrograd, Part 1</title>
      <link>http://localhost:1313/memo/posts/epoch1/</link>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/memo/posts/epoch1/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Hi! I&amp;rsquo;m Mon. I&amp;rsquo;ve decided to study AI, ML&amp;hellip; almost every day with doing practical projects.&lt;/p&gt;&#xA;&lt;p&gt;I write about them here.&lt;/p&gt;&#xA;&lt;p&gt;This is Epoch 1: the very first step.&lt;/p&gt;&#xA;&lt;h2 id=&#34;content-gradient&#34;&gt;Content: Gradient&lt;/h2&gt;&#xA;&lt;p&gt;Building a neural network; a tiny autograd engine (micrograd) — an automatic differentiation engine with backpropagation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt; (single input change → single output change): the rate of change.&lt;/p&gt;&#xA;&lt;div class=&#34;math-container&#34;&gt;&#xA;&lt;p&gt;Example: if $f(x)=x^2$, and $x$ changes by a fixed amount, say 1:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
